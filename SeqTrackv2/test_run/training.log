

=== RUN START epoch 1 seed 7 time Wed Oct 15 11:23:09 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 1 rc 2 time Wed Oct 15 11:23:10 2025 ===


=== RUN START epoch 2 seed 7 time Wed Oct 15 11:23:10 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 2 rc 2 time Wed Oct 15 11:23:10 2025 ===


=== RUN START epoch 3 seed 7 time Wed Oct 15 11:23:10 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 3 rc 2 time Wed Oct 15 11:23:10 2025 ===


=== RUN START epoch 4 seed 7 time Wed Oct 15 11:23:10 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 4 rc 2 time Wed Oct 15 11:23:10 2025 ===


=== RUN START epoch 5 seed 7 time Wed Oct 15 11:23:10 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 5 rc 2 time Wed Oct 15 11:23:10 2025 ===


=== RUN START epoch 1 seed 7 time Wed Oct 15 11:31:24 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 1 rc 2 time Wed Oct 15 11:31:35 2025 ===


=== RUN START epoch 2 seed 7 time Wed Oct 15 11:31:35 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 2 rc 2 time Wed Oct 15 11:31:37 2025 ===


=== RUN START epoch 3 seed 7 time Wed Oct 15 11:31:37 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 3 rc 2 time Wed Oct 15 11:31:39 2025 ===


=== RUN START epoch 4 seed 7 time Wed Oct 15 11:31:39 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 4 rc 2 time Wed Oct 15 11:31:41 2025 ===


=== RUN START epoch 5 seed 7 time Wed Oct 15 11:31:41 2025 ===
usage: train.py [-h] [--script SCRIPT] [--config CONFIG] [--save_dir SAVE_DIR]
                [--mode {single,multiple}] [--nproc_per_node NPROC_PER_NODE]
                [--use_lmdb {0,1}] [--distill {0,1}]
                [--script_teacher SCRIPT_TEACHER]
                [--config_teacher CONFIG_TEACHER]
train.py: error: unrecognized arguments: --seed 7

=== RUN END epoch 5 rc 2 time Wed Oct 15 11:31:43 2025 ===


=== RUN START epoch 1 time Wed Oct 15 11:41:24 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 11:41:58 2025 ===


=== RUN START epoch 2 time Wed Oct 15 11:41:58 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 11:42:07 2025 ===


=== RUN START epoch 3 time Wed Oct 15 11:42:07 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 11:42:15 2025 ===


=== RUN START epoch 4 time Wed Oct 15 11:42:15 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 11:42:23 2025 ===


=== RUN START epoch 5 time Wed Oct 15 11:42:23 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 11:42:31 2025 ===


=== RUN START epoch 1 time Wed Oct 15 11:46:17 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 11:46:24 2025 ===


=== RUN START epoch 2 time Wed Oct 15 11:46:24 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 11:46:32 2025 ===


=== RUN START epoch 3 time Wed Oct 15 11:46:32 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 11:46:40 2025 ===


=== RUN START epoch 4 time Wed Oct 15 11:46:40 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 11:46:49 2025 ===


=== RUN START epoch 5 time Wed Oct 15 11:46:49 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 11:46:58 2025 ===


=== RUN START epoch 1 time Wed Oct 15 11:51:34 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 11:51:42 2025 ===


=== RUN START epoch 2 time Wed Oct 15 11:51:42 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 11:51:50 2025 ===


=== RUN START epoch 3 time Wed Oct 15 11:51:50 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 11:51:58 2025 ===


=== RUN START epoch 4 time Wed Oct 15 11:51:58 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 11:52:06 2025 ===


=== RUN START epoch 5 time Wed Oct 15 11:52:06 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 47, in run
    loader_train = build_dataloaders(cfg, settings)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 209, in build_dataloaders
    dataset_train = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),
                                                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/base_functions.py", line 163, in names2datasets
    datasets.append(RefCOCOSeq(settings.env.refcoco_dir, split="train", image_loader=image_loader,
                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               name="refcocog", splitBy="google",
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_vision=settings.multi_modal_vision,
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               multi_modal_language=settings.multi_modal_language
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ))
                               ^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refcoco_seq.py", line 57, in __init__
    self.coco_set = REFER(root, dataset=name, splitBy=splitBy)
                    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/dataset/refer.py", line 68, in __init__
    self.data['refs'] = pickle.load(open(ref_file, 'rb'))
                                    ~~~~^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/saifm/SeqTrackv2/data/refcoco/refcocog/refs(google).p'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 11:52:14 2025 ===


=== RUN START epoch 1 time Wed Oct 15 11:55:50 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
  0%|                                                                       | 0/231508 [00:00<?, ?B/s]  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                  | 17408/231508 [00:00<00:01, 147456.26B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 69632/231508 [00:00<00:00, 292629.11B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 174080/231508 [00:00<00:00, 498571.66B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231508/231508 [00:00<00:00, 556175.39B/s]
Downloading: "https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth" to /home/saifm/.cache/torch/hub/checkpoints/mae_pretrain_vit_base.pth
  0%|                                                                    | 0/407873900 [00:00<?, ?B/s]  0%|                                                    | 17408/407873900 [00:00<56:17, 120759.28B/s]  0%|                                                    | 69632/407873900 [00:00<26:34, 255831.10B/s]  0%|                                                   | 174080/407873900 [00:00<14:41, 462447.75B/s]  0%|                                                   | 313344/407873900 [00:00<10:18, 659403.41B/s]  0%|                                                  | 574464/407873900 [00:00<06:19, 1073261.14B/s]  0%|                                                   | 783360/407873900 [00:01<07:22, 920731.49B/s]  0%|â–                                                | 1427456/407873900 [00:01<03:20, 2029689.08B/s]  0%|â–                                                | 1758208/407873900 [00:01<02:58, 2278587.25B/s]  1%|â–Ž                                                | 2176000/407873900 [00:01<02:29, 2721122.62B/s]  1%|â–Ž                                                | 2496512/407873900 [00:01<02:50, 2377836.85B/s]  1%|â–Ž                                                | 2774016/407873900 [00:01<03:09, 2139880.60B/s]  1%|â–Ž                                                | 3017728/407873900 [00:01<03:24, 1975262.31B/s]  1%|â–                                                | 3235840/407873900 [00:01<03:35, 1881387.35B/s]  1%|â–                                                | 3437568/407873900 [00:02<03:47, 1777671.73B/s]  1%|â–                                                | 3624960/407873900 [00:02<03:55, 1714621.05B/s]  1%|â–                                                | 3802112/407873900 [00:02<04:06, 1641317.93B/s]  1%|â–                                                | 3974144/407873900 [00:02<04:14, 1585085.50B/s]  1%|â–Œ                                                | 4219904/407873900 [00:02<04:10, 1612201.04B/s]  1%|â–Œ                                                | 4465664/407873900 [00:02<04:04, 1646790.95B/s]  1%|â–Œ                                                | 4711424/407873900 [00:02<03:45, 1784843.06B/s]  1%|â–Œ                                                | 4908032/407873900 [00:02<03:42, 1808201.41B/s]  1%|â–Œ                                                | 5091328/407873900 [00:03<03:46, 1775650.77B/s]  1%|â–‹                                                | 5270528/407873900 [00:03<04:00, 1676756.33B/s]  1%|â–‹                                                | 5481472/407873900 [00:03<03:58, 1688874.65B/s]  1%|â–‹                                                | 5694464/407873900 [00:03<03:44, 1790289.12B/s]  1%|â–‹                                                | 5875712/407873900 [00:03<03:46, 1772837.25B/s]  1%|â–‹                                                | 6071296/407873900 [00:03<03:42, 1805347.00B/s]  2%|â–Š                                                | 6253568/407873900 [00:03<03:52, 1729643.15B/s]  2%|â–Š                                                | 6472704/407873900 [00:03<03:36, 1857490.80B/s]  2%|â–Š                                                | 6661120/407873900 [00:03<03:42, 1801149.51B/s]  2%|â–Š                                                | 6857728/407873900 [00:04<03:40, 1820011.11B/s]  2%|â–Š                                                | 7041024/407873900 [00:04<03:51, 1731056.75B/s]  2%|â–Š                                                | 7267328/407873900 [00:04<03:42, 1800613.46B/s]  2%|â–‰                                                | 7480320/407873900 [00:04<03:32, 1883673.77B/s]  2%|â–‰                                                | 7670784/407873900 [00:04<03:35, 1856287.09B/s]  2%|â–‰                                                | 7858176/407873900 [00:04<03:42, 1798676.62B/s]  2%|â–‰                                                | 8053760/407873900 [00:04<03:51, 1723677.54B/s]  2%|â–‰                                                | 8299520/407873900 [00:04<03:30, 1894728.01B/s]  2%|â–ˆ                                                | 8491008/407873900 [00:04<03:32, 1883528.87B/s]  2%|â–ˆ                                                | 8681472/407873900 [00:05<03:36, 1841756.03B/s]  2%|â–ˆ                                                | 8866816/407873900 [00:05<03:44, 1777838.10B/s]  2%|â–ˆ                                                | 9085952/407873900 [00:05<03:33, 1871201.92B/s]  2%|â–ˆ                                                | 9274368/407873900 [00:05<03:37, 1834311.60B/s]  2%|â–ˆâ–                                               | 9462784/407873900 [00:05<03:36, 1837922.87B/s]  2%|â–ˆâ–                                               | 9647104/407873900 [00:05<03:41, 1795890.89B/s]  2%|â–ˆâ–                                               | 9856000/407873900 [00:05<03:33, 1866822.86B/s]  2%|â–ˆâ–                                              | 10052608/407873900 [00:05<03:35, 1849196.91B/s]  3%|â–ˆâ–                                              | 10249216/407873900 [00:05<03:32, 1868825.06B/s]  3%|â–ˆâ–                                              | 10436608/407873900 [00:06<03:39, 1808349.10B/s]  3%|â–ˆâ–Ž                                              | 10658816/407873900 [00:06<03:33, 1857570.80B/s]  3%|â–ˆâ–Ž                                              | 10855424/407873900 [00:06<03:31, 1872750.71B/s]  3%|â–ˆâ–Ž                                              | 11043840/407873900 [00:06<03:31, 1874311.18B/s]  3%|â–ˆâ–Ž                                              | 11232256/407873900 [00:06<03:42, 1784821.38B/s]  3%|â–ˆâ–Ž                                              | 11461632/407873900 [00:06<03:33, 1854259.91B/s]  3%|â–ˆâ–Ž                                              | 11674624/407873900 [00:06<03:32, 1867588.06B/s]  3%|â–ˆâ–                                              | 11862016/407873900 [00:06<03:33, 1853101.05B/s]  3%|â–ˆâ–                                              | 12048384/407873900 [00:06<03:33, 1852431.10B/s]  3%|â–ˆâ–                                              | 12248064/407873900 [00:06<03:35, 1838597.25B/s]  3%|â–ˆâ–                                              | 12461056/407873900 [00:07<03:27, 1907678.72B/s]  3%|â–ˆâ–                                              | 12652544/407873900 [00:07<03:32, 1856760.42B/s]  3%|â–ˆâ–Œ                                              | 12838912/407873900 [00:07<03:35, 1837337.44B/s]  3%|â–ˆâ–Œ                                              | 13041664/407873900 [00:07<03:28, 1891918.83B/s]  3%|â–ˆâ–Œ                                              | 13232128/407873900 [00:07<03:31, 1869360.03B/s]  3%|â–ˆâ–Œ                                              | 13419520/407873900 [00:07<03:36, 1820773.86B/s]  3%|â–ˆâ–Œ                                              | 13624320/407873900 [00:07<03:32, 1855508.43B/s]  3%|â–ˆâ–‹                                              | 13837312/407873900 [00:07<03:27, 1898813.21B/s]  3%|â–ˆâ–‹                                              | 14033920/407873900 [00:07<03:26, 1908189.03B/s]  3%|â–ˆâ–‹                                              | 14225408/407873900 [00:08<03:33, 1840502.30B/s]  4%|â–ˆâ–‹                                              | 14427136/407873900 [00:08<03:30, 1870249.76B/s]  4%|â–ˆâ–‹                                              | 14656512/407873900 [00:08<03:20, 1958159.40B/s]  4%|â–ˆâ–‹                                              | 14853120/407873900 [00:08<03:21, 1950108.32B/s]  4%|â–ˆâ–Š                                              | 15048704/407873900 [00:08<03:28, 1886990.15B/s]  4%|â–ˆâ–Š                                              | 15238144/407873900 [00:08<03:27, 1888629.80B/s]  4%|â–ˆâ–Š                                              | 15475712/407873900 [00:08<03:19, 1966102.36B/s]  4%|â–ˆâ–Š                                              | 15688704/407873900 [00:08<03:16, 1998400.49B/s]  4%|â–ˆâ–Š                                              | 15889408/407873900 [00:08<03:24, 1916365.04B/s]  4%|â–ˆâ–‰                                              | 16081920/407873900 [00:08<03:25, 1905825.43B/s]  4%|â–ˆâ–‰                                              | 16327680/407873900 [00:09<03:11, 2041047.44B/s]  4%|â–ˆâ–‰                                              | 16540672/407873900 [00:09<03:11, 2040035.96B/s]  4%|â–ˆâ–‰                                              | 16745472/407873900 [00:09<03:18, 1969303.40B/s]  4%|â–ˆâ–‰                                              | 16950272/407873900 [00:09<03:16, 1984401.27B/s]  4%|â–ˆâ–ˆ                                              | 17179648/407873900 [00:09<03:08, 2073571.35B/s]  4%|â–ˆâ–ˆ                                              | 17392640/407873900 [00:09<03:07, 2078280.91B/s]  4%|â–ˆâ–ˆ                                              | 17605632/407873900 [00:09<03:12, 2025698.55B/s]  4%|â–ˆâ–ˆ                                              | 17835008/407873900 [00:09<03:08, 2067246.52B/s]  4%|â–ˆâ–ˆâ–                                             | 18080768/407873900 [00:09<03:00, 2156091.92B/s]  4%|â–ˆâ–ˆâ–                                             | 18320384/407873900 [00:10<02:55, 2225385.19B/s]  5%|â–ˆâ–ˆâ–                                             | 18543616/407873900 [00:10<03:03, 2122269.87B/s]  5%|â–ˆâ–ˆâ–                                             | 18768896/407873900 [00:10<03:01, 2141917.19B/s]  5%|â–ˆâ–ˆâ–                                             | 19031040/407873900 [00:10<02:52, 2257850.11B/s]  5%|â–ˆâ–ˆâ–Ž                                             | 19276800/407873900 [00:10<02:49, 2289438.42B/s]  5%|â–ˆâ–ˆâ–Ž                                             | 19507200/407873900 [00:10<02:55, 2209606.74B/s]  5%|â–ˆâ–ˆâ–Ž                                             | 19768320/407873900 [00:10<02:51, 2266856.23B/s]  5%|â–ˆâ–ˆâ–Ž                                             | 20030464/407873900 [00:10<02:44, 2362214.71B/s]  5%|â–ˆâ–ˆâ–                                             | 20292608/407873900 [00:10<02:40, 2408342.48B/s]  5%|â–ˆâ–ˆâ–                                             | 20534272/407873900 [00:11<02:44, 2355043.76B/s]  5%|â–ˆâ–ˆâ–                                             | 20784128/407873900 [00:11<02:49, 2286271.89B/s]  5%|â–ˆâ–ˆâ–                                             | 21095424/407873900 [00:11<02:34, 2498393.88B/s]  5%|â–ˆâ–ˆâ–Œ                                             | 21373952/407873900 [00:11<02:30, 2574277.39B/s]  5%|â–ˆâ–ˆâ–Œ                                             | 21633024/407873900 [00:11<02:33, 2514367.82B/s]  5%|â–ˆâ–ˆâ–Œ                                             | 21885952/407873900 [00:11<02:38, 2432209.53B/s]  5%|â–ˆâ–ˆâ–Œ                                             | 22209536/407873900 [00:11<02:27, 2622210.81B/s]  6%|â–ˆâ–ˆâ–‹                                             | 22513664/407873900 [00:11<02:20, 2741131.46B/s]  6%|â–ˆâ–ˆâ–‹                                             | 22790144/407873900 [00:11<02:24, 2673562.35B/s]  6%|â–ˆâ–ˆâ–‹                                             | 23061504/407873900 [00:11<02:29, 2573970.42B/s]  6%|â–ˆâ–ˆâ–Š                                             | 23421952/407873900 [00:12<02:15, 2837055.18B/s]  6%|â–ˆâ–ˆâ–Š                                             | 23716864/407873900 [00:12<02:14, 2849909.54B/s]  6%|â–ˆâ–ˆâ–Š                                             | 24003584/407873900 [00:12<02:16, 2821442.41B/s]  6%|â–ˆâ–ˆâ–Š                                             | 24323072/407873900 [00:12<02:14, 2859536.80B/s]  6%|â–ˆâ–ˆâ–‰                                             | 24664064/407873900 [00:12<02:07, 3017355.79B/s]  6%|â–ˆâ–ˆâ–‰                                             | 24967168/407873900 [00:12<02:08, 2984075.31B/s]  6%|â–ˆâ–ˆâ–‰                                             | 25289728/407873900 [00:12<02:06, 3035259.82B/s]  6%|â–ˆâ–ˆâ–ˆ                                             | 25650176/407873900 [00:12<02:00, 3167623.05B/s]  6%|â–ˆâ–ˆâ–ˆ                                             | 25967616/407873900 [00:12<02:05, 3042664.16B/s]  6%|â–ˆâ–ˆâ–ˆ                                             | 26305536/407873900 [00:13<02:02, 3125704.33B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 26682368/407873900 [00:13<01:55, 3302341.97B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 27042816/407873900 [00:13<01:53, 3353254.40B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 27403264/407873900 [00:13<01:51, 3408348.95B/s]  7%|â–ˆâ–ˆâ–ˆâ–Ž                                            | 27782144/407873900 [00:13<01:47, 3519599.05B/s]  7%|â–ˆâ–ˆâ–ˆâ–Ž                                            | 28156928/407873900 [00:13<01:45, 3582436.26B/s]  7%|â–ˆâ–ˆâ–ˆâ–Ž                                            | 28550144/407873900 [00:13<01:43, 3649788.69B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 28926976/407873900 [00:13<01:42, 3682094.94B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 29317120/407873900 [00:13<01:41, 3746881.19B/s]  7%|â–ˆâ–ˆâ–ˆâ–                                            | 29693952/407873900 [00:13<01:40, 3751101.25B/s]  7%|â–ˆâ–ˆâ–ˆâ–Œ                                            | 30069760/407873900 [00:14<01:40, 3749360.85B/s]  7%|â–ˆâ–ˆâ–ˆâ–Œ                                            | 30450688/407873900 [00:14<01:41, 3723901.65B/s]  8%|â–ˆâ–ˆâ–ˆâ–‹                                            | 30827520/407873900 [00:14<01:40, 3735036.01B/s]  8%|â–ˆâ–ˆâ–ˆâ–‹                                            | 31201280/407873900 [00:14<01:52, 3344283.67B/s]  8%|â–ˆâ–ˆâ–ˆâ–‹                                            | 31591424/407873900 [00:14<01:47, 3497737.96B/s]  8%|â–ˆâ–ˆâ–ˆâ–Š                                            | 31959040/407873900 [00:14<01:45, 3547141.57B/s]  8%|â–ˆâ–ˆâ–ˆâ–Š                                            | 32334848/407873900 [00:14<01:44, 3603976.05B/s]  8%|â–ˆâ–ˆâ–ˆâ–Š                                            | 32711680/407873900 [00:14<01:42, 3649057.08B/s]  8%|â–ˆâ–ˆâ–ˆâ–‰                                            | 33089536/407873900 [00:14<01:41, 3686480.21B/s]  8%|â–ˆâ–ˆâ–ˆâ–‰                                            | 33465344/407873900 [00:14<01:41, 3700879.85B/s]  8%|â–ˆâ–ˆâ–ˆâ–‰                                            | 33842176/407873900 [00:15<01:40, 3711551.29B/s]  8%|â–ˆâ–ˆâ–ˆâ–ˆ                                            | 34235392/407873900 [00:15<01:39, 3737003.85B/s]  8%|â–ˆâ–ˆâ–ˆâ–ˆ                                            | 34612224/407873900 [00:15<01:40, 3718631.38B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆ                                            | 34989056/407873900 [00:15<01:40, 3709492.97B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 35365888/407873900 [00:15<01:40, 3702093.83B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 35742720/407873900 [00:15<01:40, 3709378.93B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 36119552/407873900 [00:15<01:40, 3709561.47B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 36496384/407873900 [00:15<01:40, 3690842.56B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 36873216/407873900 [00:15<01:39, 3711946.01B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 37250048/407873900 [00:15<01:39, 3707499.16B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 37626880/407873900 [00:16<01:39, 3709845.14B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 38003712/407873900 [00:16<01:39, 3703620.94B/s]  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 38380544/407873900 [00:16<01:39, 3701689.26B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 38773760/407873900 [00:16<01:38, 3733903.42B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 39147520/407873900 [00:16<01:38, 3729741.09B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 39527424/407873900 [00:16<01:39, 3701470.34B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 39920640/407873900 [00:16<01:38, 3719194.66B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 40307712/407873900 [00:16<01:37, 3763547.62B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 40684544/407873900 [00:16<01:37, 3760077.52B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 41061376/407873900 [00:17<01:38, 3716224.08B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                           | 41437184/407873900 [00:17<01:38, 3728490.08B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                           | 41810944/407873900 [00:17<03:47, 1605725.29B/s] 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 42738688/407873900 [00:17<02:15, 2691919.70B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 43784192/407873900 [00:17<01:29, 4051949.96B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 44393472/407873900 [00:18<01:43, 3527935.11B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                          | 45016064/407873900 [00:18<01:33, 3872366.41B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                          | 45523968/407873900 [00:18<01:34, 3815441.18B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 45988864/407873900 [00:18<01:54, 3154494.60B/s] 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 46638080/407873900 [00:18<01:36, 3747484.55B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                          | 47091712/407873900 [00:18<01:43, 3495705.14B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                          | 47496192/407873900 [00:19<01:49, 3303055.76B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 47863808/407873900 [00:19<01:52, 3212473.96B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 48209920/407873900 [00:19<01:54, 3149750.73B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 48541696/407873900 [00:19<01:57, 3046832.18B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 48857088/407873900 [00:19<01:59, 3007580.26B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 49177600/407873900 [00:19<01:58, 3027804.92B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 49510400/407873900 [00:19<01:55, 3106912.44B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 49825792/407873900 [00:19<01:57, 3056235.52B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 50135040/407873900 [00:19<01:57, 3035891.01B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 50455552/407873900 [00:20<01:56, 3073547.90B/s] 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 50783232/407873900 [00:20<01:55, 3097322.76B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 51094528/407873900 [00:20<01:55, 3081249.03B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 51403776/407873900 [00:20<01:55, 3074241.08B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 51766272/407873900 [00:20<01:51, 3202992.02B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 52087808/407873900 [00:20<01:51, 3203355.83B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 52409344/407873900 [00:20<01:54, 3105132.49B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 52732928/407873900 [00:20<01:53, 3140844.80B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 53093376/407873900 [00:20<01:48, 3258191.72B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                         | 53420032/407873900 [00:20<01:49, 3247875.80B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                         | 53745664/407873900 [00:21<01:50, 3198565.20B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                         | 54092800/407873900 [00:21<01:49, 3219405.11B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 54436864/407873900 [00:21<01:48, 3260610.94B/s] 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 54763520/407873900 [00:21<01:48, 3256209.90B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 55090176/407873900 [00:21<02:40, 2195138.34B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                         | 55731200/407873900 [00:21<02:03, 2858239.32B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                         | 56075264/407873900 [00:21<02:08, 2733026.54B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 56402944/407873900 [00:22<02:14, 2605781.88B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 56747008/407873900 [00:22<02:17, 2550268.58B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 57107456/407873900 [00:22<02:17, 2547670.02B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 57467904/407873900 [00:22<02:18, 2539152.41B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 57811968/407873900 [00:22<02:13, 2630225.20B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 58080256/407873900 [00:22<02:13, 2611500.98B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 58345472/407873900 [00:22<02:15, 2580615.82B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                         | 58605568/407873900 [00:22<02:18, 2514539.47B/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                         | 58926080/407873900 [00:23<02:18, 2518594.21B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                         | 59251712/407873900 [00:23<02:08, 2712160.70B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 59526144/407873900 [00:23<02:09, 2679819.80B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 59796480/407873900 [00:23<02:15, 2573408.59B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 60056576/407873900 [00:23<02:15, 2561592.94B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 60351488/407873900 [00:23<02:10, 2663627.20B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 60646400/407873900 [00:23<02:07, 2723396.58B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 60920832/407873900 [00:23<02:11, 2633877.65B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 61219840/407873900 [00:23<02:11, 2636113.92B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 61531136/407873900 [00:23<02:06, 2735072.21B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                        | 61809664/407873900 [00:24<02:07, 2723897.21B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                        | 62083072/407873900 [00:24<02:07, 2703544.81B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                        | 62383104/407873900 [00:24<02:07, 2704029.20B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 62694400/407873900 [00:24<02:03, 2806189.42B/s] 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 62976000/407873900 [00:24<02:05, 2751873.75B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 63252480/407873900 [00:24<02:09, 2668422.57B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 63562752/407873900 [00:24<02:07, 2702820.52B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 63890432/407873900 [00:24<02:00, 2847271.09B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 64176128/407873900 [00:24<02:00, 2848620.52B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 64461824/407873900 [00:25<02:04, 2763815.86B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 64775168/407873900 [00:25<02:04, 2746017.32B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 65086464/407873900 [00:25<02:01, 2832099.54B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 65381376/407873900 [00:25<02:01, 2824179.87B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 65665024/407873900 [00:25<02:04, 2738751.42B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                        | 65971200/407873900 [00:25<02:03, 2778719.10B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                        | 66282496/407873900 [00:25<01:59, 2866906.83B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                        | 66570240/407873900 [00:25<02:00, 2841671.54B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                        | 66854912/407873900 [00:25<02:02, 2781487.23B/s] 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                        | 67167232/407873900 [00:26<02:03, 2752855.54B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                        | 67494912/407873900 [00:26<01:58, 2876291.81B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                        | 67783680/407873900 [00:26<01:59, 2850607.19B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 68069376/407873900 [00:26<02:02, 2775217.94B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 68379648/407873900 [00:26<02:03, 2759705.00B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 68707328/407873900 [00:26<01:56, 2899655.35B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 68999168/407873900 [00:26<01:58, 2858021.43B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 69285888/407873900 [00:26<02:02, 2766305.19B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 69575680/407873900 [00:26<02:03, 2731879.10B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 69903360/407873900 [00:26<01:57, 2885663.07B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 70194176/407873900 [00:27<01:56, 2891915.31B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 70484992/407873900 [00:27<02:00, 2808318.45B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 70788096/407873900 [00:27<02:02, 2740866.21B/s] 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 71132160/407873900 [00:27<01:55, 2906325.78B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 71425024/407873900 [00:27<01:57, 2854342.17B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 71711744/407873900 [00:27<02:02, 2734217.13B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 72000512/407873900 [00:27<02:02, 2750728.59B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 72328192/407873900 [00:27<01:57, 2852623.25B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 72623104/407873900 [00:27<01:57, 2843148.99B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 72908800/407873900 [00:28<02:00, 2788479.77B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 73188352/407873900 [00:28<02:01, 2754097.50B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 73507840/407873900 [00:28<01:56, 2878076.83B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 73796608/407873900 [00:28<02:06, 2644824.19B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 74064896/407873900 [00:28<03:12, 1732269.64B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                       | 74753024/407873900 [00:28<02:00, 2771333.20B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                       | 75105280/407873900 [00:28<02:13, 2489335.96B/s] 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                       | 75410432/407873900 [00:29<02:20, 2372789.62B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 75686912/407873900 [00:29<02:24, 2303472.03B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 75949056/407873900 [00:29<02:24, 2294148.13B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 76211200/407873900 [00:29<02:20, 2356246.45B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 76461056/407873900 [00:29<02:22, 2318396.68B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 76702720/407873900 [00:29<02:26, 2255839.19B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 76935168/407873900 [00:29<02:27, 2242068.18B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 77227008/407873900 [00:29<02:17, 2405041.92B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 77472768/407873900 [00:29<02:17, 2397281.08B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 77716480/407873900 [00:30<02:20, 2343058.08B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 77953024/407873900 [00:30<02:22, 2317116.27B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 78259200/407873900 [00:30<02:10, 2525311.26B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 78514176/407873900 [00:30<02:12, 2478100.69B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 78764032/407873900 [00:30<02:16, 2415122.96B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 79029248/407873900 [00:30<02:15, 2432382.61B/s] 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 79340544/407873900 [00:30<02:06, 2601619.79B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 79602688/407873900 [00:30<02:08, 2557007.19B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 79859712/407873900 [00:30<02:09, 2536083.39B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 80143360/407873900 [00:31<02:08, 2551760.69B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 80454656/407873900 [00:31<02:01, 2693960.07B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 80724992/407873900 [00:31<02:03, 2644648.00B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 80990208/407873900 [00:31<02:06, 2577871.36B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 81290240/407873900 [00:31<02:03, 2634433.83B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 81601536/407873900 [00:31<01:58, 2753460.09B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 81878016/407873900 [00:31<01:58, 2746168.65B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 82153472/407873900 [00:31<02:03, 2643361.89B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 82453504/407873900 [00:31<02:00, 2699365.38B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 82749440/407873900 [00:32<01:57, 2773290.35B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 83027968/407873900 [00:32<02:00, 2701008.59B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 83305472/407873900 [00:32<01:59, 2711052.06B/s] 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 83600384/407873900 [00:32<01:56, 2774830.07B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 83884032/407873900 [00:32<01:56, 2792837.35B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 84164608/407873900 [00:32<01:56, 2775857.53B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 84452352/407873900 [00:32<01:56, 2769946.98B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 84763648/407873900 [00:32<01:53, 2836629.47B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 85048320/407873900 [00:32<01:57, 2753633.60B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 85353472/407873900 [00:32<01:55, 2804118.65B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 85664768/407873900 [00:33<01:53, 2837678.21B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 85959680/407873900 [00:33<01:52, 2867878.77B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 86247424/407873900 [00:33<01:55, 2786766.21B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 86565888/407873900 [00:33<01:54, 2808803.58B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 86877184/407873900 [00:33<01:51, 2888793.73B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                     | 87166976/407873900 [00:33<01:51, 2871220.84B/s] 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                     | 87454720/407873900 [00:33<01:53, 2815252.84B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                     | 87746560/407873900 [00:33<01:52, 2844270.71B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                     | 88056832/407873900 [00:33<01:50, 2891913.67B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 88346624/407873900 [00:34<01:52, 2830897.04B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 88646656/407873900 [00:34<01:54, 2798528.08B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 88941568/407873900 [00:34<01:52, 2826001.09B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 89225216/407873900 [00:34<01:55, 2753257.62B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 89531392/407873900 [00:34<01:52, 2833251.93B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 89816064/407873900 [00:34<01:52, 2823308.88B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 90121216/407873900 [00:34<01:51, 2858456.91B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 90416128/407873900 [00:34<01:52, 2815182.73B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 90698752/407873900 [00:34<01:54, 2780166.56B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 91022336/407873900 [00:34<01:51, 2848985.86B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 91333632/407873900 [00:35<01:48, 2906283.94B/s] 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 91624448/407873900 [00:35<01:53, 2786680.63B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 91907072/407873900 [00:35<01:53, 2786371.15B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 92251136/407873900 [00:35<01:47, 2944455.02B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 92547072/407873900 [00:35<01:50, 2844341.06B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 92832768/407873900 [00:35<01:52, 2805800.80B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 93119488/407873900 [00:35<01:51, 2812165.99B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 93447168/407873900 [00:35<01:46, 2945015.19B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 93743104/407873900 [00:35<01:51, 2825086.77B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 94036992/407873900 [00:36<01:50, 2836092.23B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 94331904/407873900 [00:36<01:51, 2818002.13B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 94614528/407873900 [00:36<01:52, 2783666.28B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 94921728/407873900 [00:36<01:52, 2781497.91B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 95233024/407873900 [00:36<01:49, 2849251.49B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 95544320/407873900 [00:36<01:47, 2896281.21B/s] 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 95835136/407873900 [00:36<01:48, 2880821.77B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 96123904/407873900 [00:36<01:49, 2858767.34B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 96427008/407873900 [00:36<01:47, 2908984.61B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 96723968/407873900 [00:36<01:47, 2891829.55B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 97018880/407873900 [00:37<01:48, 2857744.19B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 97330176/407873900 [00:37<01:47, 2897756.84B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 97620992/407873900 [00:37<01:47, 2896626.02B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 97920000/407873900 [00:37<01:46, 2906311.89B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 98214912/407873900 [00:37<01:46, 2894915.05B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 98526208/407873900 [00:37<01:46, 2916240.64B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 98837504/407873900 [00:37<01:44, 2963772.05B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 99134464/407873900 [00:37<01:45, 2915172.41B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 99443712/407873900 [00:37<01:44, 2956426.29B/s] 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 99755008/407873900 [00:37<01:44, 2960477.43B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 100051968/407873900 [00:38<01:44, 2951546.54B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 100361216/407873900 [00:38<01:43, 2982297.41B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 100672512/407873900 [00:38<01:43, 2972689.01B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 101000192/407873900 [00:38<01:41, 3024537.44B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 101311488/407873900 [00:38<01:41, 3023653.52B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 101622784/407873900 [00:38<01:41, 3015139.04B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 101950464/407873900 [00:38<01:40, 3053943.64B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 102278144/407873900 [00:38<01:39, 3070199.51B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 102605824/407873900 [00:38<01:38, 3113718.19B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 102918144/407873900 [00:39<01:39, 3059227.85B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                   | 103277568/407873900 [00:39<01:34, 3207876.04B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                   | 103605248/407873900 [00:39<01:35, 3172831.93B/s] 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                   | 103932928/407873900 [00:39<01:35, 3170948.01B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 104260608/407873900 [00:39<01:35, 3162693.89B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 104621056/407873900 [00:39<01:32, 3281418.61B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 104949760/407873900 [00:39<01:33, 3235558.83B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 105287680/407873900 [00:39<01:32, 3277459.97B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 105620480/407873900 [00:39<01:31, 3286604.03B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 105964544/407873900 [00:39<01:31, 3312171.44B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 106308608/407873900 [00:40<01:30, 3322295.45B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 106652672/407873900 [00:40<01:30, 3325126.97B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 107029504/407873900 [00:40<01:27, 3427433.53B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 107373568/407873900 [00:40<01:27, 3420722.85B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 107716608/407873900 [00:40<01:30, 3315837.03B/s] 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 108049408/407873900 [00:40<01:31, 3260686.93B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 108405760/407873900 [00:40<01:29, 3341872.32B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 108782592/407873900 [00:40<01:27, 3424854.01B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 109159424/407873900 [00:40<01:25, 3489212.42B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 109541376/407873900 [00:40<01:23, 3584041.22B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 109900800/407873900 [00:41<01:23, 3574072.88B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 110289920/407873900 [00:41<01:22, 3608407.92B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 110683136/407873900 [00:41<01:20, 3676571.01B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 111076352/407873900 [00:41<01:20, 3707001.08B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 111469568/407873900 [00:41<01:19, 3729268.85B/s] 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 111847424/407873900 [00:41<01:19, 3743309.83B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 112253952/407873900 [00:41<01:17, 3838205.00B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 112638976/407873900 [00:41<01:18, 3772614.02B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 113026048/407873900 [00:41<01:17, 3789925.48B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 113405952/407873900 [00:42<01:17, 3784185.24B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 113784832/407873900 [00:42<01:17, 3783944.73B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 114163712/407873900 [00:42<01:17, 3779579.15B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 114542592/407873900 [00:42<01:17, 3770309.52B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 114920448/407873900 [00:42<01:18, 3745240.63B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 115303424/407873900 [00:42<01:18, 3738422.71B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 115693568/407873900 [00:42<01:17, 3786519.40B/s] 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 116073472/407873900 [00:42<01:17, 3748645.05B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 116450304/407873900 [00:42<01:18, 3706093.11B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 116843520/407873900 [00:42<01:17, 3736657.51B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 117236736/407873900 [00:43<01:17, 3754095.62B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 117613568/407873900 [00:43<01:17, 3752098.41B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 118006784/407873900 [00:43<01:16, 3768167.98B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 118384640/407873900 [00:43<01:18, 3710245.49B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 118776832/407873900 [00:43<01:17, 3735053.18B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 119153664/407873900 [00:43<01:17, 3715091.45B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 119530496/407873900 [00:43<01:17, 3711945.33B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 119907328/407873900 [00:43<01:18, 3686813.87B/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 120300544/407873900 [00:43<01:16, 3753546.10B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 120677376/407873900 [00:43<01:16, 3733118.99B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 121054208/407873900 [00:44<01:17, 3721357.04B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 121431040/407873900 [00:44<01:17, 3700920.21B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 121807872/407873900 [00:44<01:17, 3706624.53B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 122201088/407873900 [00:44<01:16, 3752391.58B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 122576896/407873900 [00:44<01:16, 3740928.60B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 122951680/407873900 [00:44<01:16, 3739709.64B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 123329536/407873900 [00:44<01:15, 3751193.16B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 123705344/407873900 [00:44<01:16, 3728478.40B/s] 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 124079104/407873900 [00:44<01:17, 3684729.31B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 124462080/407873900 [00:44<01:16, 3686734.56B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 124855296/407873900 [00:45<01:16, 3714531.83B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 125232128/407873900 [00:45<01:16, 3686719.12B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 125608960/407873900 [00:45<01:16, 3675040.92B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 126002176/407873900 [00:45<01:16, 3698304.29B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 126395392/407873900 [00:45<01:15, 3717686.06B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 126768128/407873900 [00:46<03:25, 1369979.06B/s] 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 127454208/407873900 [00:46<02:13, 2100696.76B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 128656384/407873900 [00:46<01:38, 2839678.63B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 130032640/407873900 [00:46<01:09, 3980609.92B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 130671616/407873900 [00:46<01:12, 3810407.97B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 131458048/407873900 [00:47<01:03, 4371199.59B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 131982336/407873900 [00:47<01:10, 3893341.95B/s] 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 132433920/407873900 [00:47<01:16, 3623805.60B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 132836352/407873900 [00:47<01:21, 3383176.14B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 133199872/407873900 [00:47<01:25, 3229573.30B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 133537792/407873900 [00:47<01:26, 3189888.21B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 133865472/407873900 [00:47<01:25, 3187811.01B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 134191104/407873900 [00:48<01:29, 3063894.95B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 134501376/407873900 [00:48<01:29, 3045824.98B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 134808576/407873900 [00:48<01:31, 2989511.29B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 135128064/407873900 [00:48<01:30, 3004027.28B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 135430144/407873900 [00:48<01:32, 2941148.23B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 135725056/407873900 [00:48<01:41, 2694077.66B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 135998464/407873900 [00:48<02:45, 1644192.97B/s] 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 136635392/407873900 [00:49<01:47, 2524634.19B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 136974336/407873900 [00:50<05:27, 827767.71B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 137221120/407873900 [00:50<04:54, 918738.79B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 137439232/407873900 [00:50<05:06, 882928.02B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 137716736/407873900 [00:50<04:13, 1067583.41B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 137909248/407873900 [00:50<04:03, 1109853.41B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 138083328/407873900 [00:51<04:15, 1054877.35B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 138241024/407873900 [00:51<04:10, 1074619.70B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 138380288/407873900 [00:51<04:15, 1056274.82B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 138535936/407873900 [00:51<04:12, 1068755.31B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 138699776/407873900 [00:51<03:58, 1130702.97B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 138880000/407873900 [00:51<03:53, 1150140.52B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139076608/407873900 [00:51<03:40, 1218655.78B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139240448/407873900 [00:52<03:38, 1231751.30B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139420672/407873900 [00:52<03:17, 1356881.88B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139564032/407873900 [00:52<03:26, 1301921.27B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139731968/407873900 [00:52<03:30, 1276581.63B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 139912192/407873900 [00:52<03:13, 1384977.75B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140055552/407873900 [00:52<03:12, 1394124.21B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140223488/407873900 [00:52<03:03, 1459299.80B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140372992/407873900 [00:52<03:13, 1384452.15B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140518400/407873900 [00:52<03:11, 1398404.88B/s] 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140698624/407873900 [00:53<03:05, 1438374.32B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 140878848/407873900 [00:53<02:56, 1515174.45B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141032448/407873900 [00:53<03:00, 1477528.34B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141181952/407873900 [00:53<03:10, 1402177.76B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141337600/407873900 [00:53<03:05, 1436924.35B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141534208/407873900 [00:53<03:03, 1449843.37B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141747200/407873900 [00:53<02:53, 1535485.70B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 141927424/407873900 [00:53<02:47, 1590408.23B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 142087168/407873900 [00:53<02:55, 1516576.29B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 142239744/407873900 [00:54<02:54, 1518595.28B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 142402560/407873900 [00:54<03:02, 1451240.04B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 142599168/407873900 [00:54<02:56, 1504615.91B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 142779392/407873900 [00:54<02:51, 1549497.06B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 142935040/407873900 [00:54<02:53, 1522891.17B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 143107072/407873900 [00:54<02:51, 1546303.50B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 143270912/407873900 [00:54<02:52, 1535357.06B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 143451136/407873900 [00:54<02:48, 1573243.42B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 143608832/407873900 [00:54<02:51, 1543401.30B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 143763456/407873900 [00:55<02:53, 1522249.00B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 143926272/407873900 [00:55<02:55, 1500246.89B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 144106496/407873900 [00:55<02:47, 1575004.23B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 144265216/407873900 [00:55<02:48, 1561505.25B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 144421888/407873900 [00:55<02:52, 1524396.49B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 144581632/407873900 [00:55<02:55, 1504384.64B/s] 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 144778240/407873900 [00:55<02:49, 1552820.95B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 144942080/407873900 [00:55<02:47, 1572175.68B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 145099776/407873900 [00:55<02:51, 1528410.26B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 145253376/407873900 [00:56<02:54, 1501160.44B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 145433600/407873900 [00:56<02:49, 1552540.78B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 145613824/407873900 [00:56<02:44, 1596107.71B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 145773568/407873900 [00:56<02:48, 1556104.13B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 145930240/407873900 [00:56<02:53, 1512998.15B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 146105344/407873900 [00:56<02:52, 1518042.90B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 146269184/407873900 [00:56<02:51, 1525240.92B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 146449408/407873900 [00:56<02:44, 1589167.05B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 146609152/407873900 [00:56<02:50, 1528017.47B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 146762752/407873900 [00:57<02:51, 1521048.80B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 146940928/407873900 [00:57<02:47, 1560765.31B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 147104768/407873900 [00:57<02:44, 1582353.02B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 147263488/407873900 [00:57<02:49, 1539689.59B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 147432448/407873900 [00:57<02:47, 1558989.81B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 147596288/407873900 [00:57<02:47, 1552776.86B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 147776512/407873900 [00:57<02:40, 1615637.64B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 147939328/407873900 [00:57<02:43, 1585699.47B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 148104192/407873900 [00:57<02:43, 1590686.20B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 148268032/407873900 [00:57<02:45, 1568069.22B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 148464640/407873900 [00:58<02:39, 1627426.49B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 148627456/407873900 [00:58<02:40, 1613683.79B/s] 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 148789248/407873900 [00:58<02:40, 1614229.34B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 148956160/407873900 [00:58<02:40, 1610439.44B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 149136384/407873900 [00:58<02:38, 1630778.54B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 149300224/407873900 [00:58<02:40, 1608298.70B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 149480448/407873900 [00:58<02:35, 1657858.13B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 149646336/407873900 [00:58<02:36, 1646916.17B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 149824512/407873900 [00:58<02:33, 1676157.21B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 149992448/407873900 [00:59<02:34, 1667622.70B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 150168576/407873900 [00:59<02:32, 1685879.64B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 150365184/407873900 [00:59<02:31, 1697985.46B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 150561792/407873900 [00:59<02:25, 1766191.72B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 150738944/407873900 [00:59<02:27, 1745111.39B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 150938624/407873900 [00:59<02:26, 1753450.44B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 151135232/407873900 [00:59<02:24, 1781727.58B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 151331840/407873900 [00:59<02:21, 1807081.14B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 151513088/407873900 [00:59<02:22, 1800968.90B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 151693312/407873900 [00:59<02:23, 1785869.20B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 151905280/407873900 [01:00<02:16, 1868712.64B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 152092672/407873900 [01:00<02:23, 1779233.35B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 152282112/407873900 [01:00<02:21, 1806289.59B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 152527872/407873900 [01:00<02:13, 1914324.04B/s] 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 152757248/407873900 [01:00<02:10, 1962104.44B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 152953856/407873900 [01:00<02:10, 1948312.89B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 153166848/407873900 [01:00<02:11, 1930153.47B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 153412608/407873900 [01:00<02:03, 2066246.25B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 153625600/407873900 [01:00<02:04, 2050043.17B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 153854976/407873900 [01:01<02:00, 2105200.64B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 154084352/407873900 [01:01<01:58, 2148504.07B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 154330112/407873900 [01:01<01:54, 2216490.27B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 154559488/407873900 [01:01<01:54, 2209973.48B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 154788864/407873900 [01:01<01:55, 2188588.03B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 155067392/407873900 [01:01<01:50, 2288447.19B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 155345920/407873900 [01:01<01:44, 2416095.73B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 155588608/407873900 [01:01<01:44, 2415695.96B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 155831296/407873900 [01:01<01:46, 2371586.52B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 156099584/407873900 [01:01<01:42, 2457157.48B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 156394496/407873900 [01:02<01:38, 2560910.70B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 156673024/407873900 [01:02<01:37, 2581738.35B/s] 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 156932096/407873900 [01:02<01:37, 2577747.30B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 157230080/407873900 [01:02<01:34, 2654653.34B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 157541376/407873900 [01:02<01:29, 2787756.16B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 157820928/407873900 [01:02<01:30, 2753303.64B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 158114816/407873900 [01:02<01:29, 2802538.43B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 158459904/407873900 [01:02<01:23, 2992747.97B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 158759936/407873900 [01:02<01:23, 2977935.78B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 159058944/407873900 [01:03<01:23, 2968785.43B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 159392768/407873900 [01:03<01:21, 3063574.48B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 159753216/407873900 [01:03<01:18, 3165845.54B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 160097280/407873900 [01:03<01:17, 3192102.83B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 160457728/407873900 [01:03<01:15, 3289031.38B/s] 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 160834560/407873900 [01:03<01:13, 3350402.91B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 161211392/407873900 [01:03<01:11, 3457669.18B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 161588224/407873900 [01:03<01:09, 3529034.72B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 161965056/407873900 [01:03<01:08, 3571044.04B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 162341888/407873900 [01:03<01:08, 3607583.55B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 162735104/407873900 [01:04<01:06, 3690557.46B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 163128320/407873900 [01:04<01:06, 3708041.05B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 163505152/407873900 [01:04<01:06, 3693151.32B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 163914752/407873900 [01:04<01:04, 3809609.53B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 164296704/407873900 [01:04<01:04, 3759627.70B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 164684800/407873900 [01:04<01:04, 3769994.76B/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 165078016/407873900 [01:04<01:04, 3785205.50B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 165471232/407873900 [01:04<01:03, 3801153.02B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 165852160/407873900 [01:04<01:03, 3797482.16B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 166241280/407873900 [01:04<01:03, 3778015.62B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 166620160/407873900 [01:05<01:04, 3736074.36B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 166993920/407873900 [01:05<01:10, 3396339.75B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 167371776/407873900 [01:05<01:09, 3469685.22B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 167724032/407873900 [01:05<01:10, 3416300.50B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 168069120/407873900 [01:05<01:13, 3284628.93B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 168400896/407873900 [01:05<01:16, 3140455.56B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 168718336/407873900 [01:05<01:19, 2992651.92B/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 169059328/407873900 [01:05<01:17, 3081534.82B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 169428992/407873900 [01:05<01:13, 3252098.41B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 169757696/407873900 [01:06<01:16, 3122566.15B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 170073088/407873900 [01:06<01:16, 3116582.38B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 170387456/407873900 [01:06<03:13, 1226926.40B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 171951104/407873900 [01:06<01:13, 3211589.26B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 172583936/407873900 [01:07<01:18, 3004615.73B/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 173105152/407873900 [01:07<01:18, 2985998.77B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 173557760/407873900 [01:07<01:21, 2874073.68B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 173952000/407873900 [01:07<01:55, 2031318.67B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 174696448/407873900 [01:08<01:23, 2803060.42B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 175133696/407873900 [01:08<01:29, 2597622.12B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 175503360/407873900 [01:08<01:33, 2483267.99B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 175826944/407873900 [01:08<01:35, 2439781.57B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 176122880/407873900 [01:08<01:36, 2408076.60B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 176399360/407873900 [01:08<01:37, 2370343.22B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 176660480/407873900 [01:08<01:38, 2357662.95B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 176913408/407873900 [01:09<01:40, 2300305.86B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 177154048/407873900 [01:09<01:42, 2254304.99B/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 177398784/407873900 [01:09<01:40, 2293238.74B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 177660928/407873900 [01:09<01:38, 2349055.71B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 177900544/407873900 [01:09<01:40, 2298233.57B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 178134016/407873900 [01:09<01:44, 2209034.85B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 178357248/407873900 [01:09<02:46, 1380057.89B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 178922496/407873900 [01:09<01:45, 2177394.29B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 179203072/407873900 [01:10<01:51, 2052892.22B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 179452928/407873900 [01:10<01:53, 2021122.39B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 179685376/407873900 [01:10<01:57, 1949200.56B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 179901440/407873900 [01:10<01:57, 1936837.55B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 180118528/407873900 [01:10<01:56, 1958306.59B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 180325376/407873900 [01:10<01:56, 1945719.67B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 180527104/407873900 [01:10<01:59, 1903464.68B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 180722688/407873900 [01:10<02:01, 1864696.80B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 180954112/407873900 [01:11<01:59, 1904285.29B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 181199872/407873900 [01:11<01:53, 1993610.69B/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 181401600/407873900 [01:11<01:55, 1960579.80B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 181599232/407873900 [01:11<01:57, 1918793.39B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 181806080/407873900 [01:11<01:57, 1923270.89B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 182051840/407873900 [01:11<01:51, 2028233.86B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 182255616/407873900 [01:11<01:51, 2027516.70B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 182461440/407873900 [01:11<01:53, 1979417.33B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 182660096/407873900 [01:11<01:55, 1947395.62B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 182920192/407873900 [01:12<01:49, 2049202.38B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 183124992/407873900 [01:12<01:52, 2004213.57B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 183346176/407873900 [01:12<01:48, 2061951.88B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 183553024/407873900 [01:12<01:49, 2054110.18B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 183788544/407873900 [01:12<01:45, 2122050.28B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 184001536/407873900 [01:12<01:49, 2037283.17B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 184230912/407873900 [01:12<01:49, 2048053.93B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 184443904/407873900 [01:12<01:47, 2069101.04B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 184673280/407873900 [01:12<01:45, 2125341.43B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 184887296/407873900 [01:13<01:47, 2077372.79B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 185115648/407873900 [01:13<01:45, 2110930.68B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 185328640/407873900 [01:13<01:47, 2074526.90B/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 185574400/407873900 [01:13<01:42, 2162843.45B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 185791488/407873900 [01:13<01:45, 2110735.68B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 186003456/407873900 [01:13<01:45, 2110759.87B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 186215424/407873900 [01:13<01:46, 2073225.58B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 186459136/407873900 [01:13<01:44, 2117995.72B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 186672128/407873900 [01:13<01:45, 2104051.14B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 186901504/407873900 [01:13<01:47, 2050544.22B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 187147264/407873900 [01:14<01:43, 2142899.44B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 187362304/407873900 [01:14<01:43, 2122073.93B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 187575296/407873900 [01:14<01:45, 2086118.14B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 187802624/407873900 [01:14<01:44, 2102681.41B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 188032000/407873900 [01:14<01:43, 2132430.06B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 188261376/407873900 [01:14<01:43, 2128563.43B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 188474368/407873900 [01:14<01:45, 2088874.12B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 188688384/407873900 [01:14<01:44, 2103186.63B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 188913664/407873900 [01:14<01:42, 2146635.30B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 189129728/407873900 [01:15<01:42, 2132166.28B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 189343744/407873900 [01:15<01:43, 2107501.46B/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 189555712/407873900 [01:15<01:44, 2079997.53B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 189785088/407873900 [01:15<01:42, 2128872.33B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 189999104/407873900 [01:15<01:42, 2117955.15B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 190211072/407873900 [01:15<01:44, 2083341.39B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 190440448/407873900 [01:15<01:44, 2090473.15B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 190679040/407873900 [01:15<01:39, 2175694.98B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 190897152/407873900 [01:15<01:40, 2163911.30B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 191114240/407873900 [01:15<01:43, 2103988.96B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 191341568/407873900 [01:16<01:43, 2082891.69B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 191587328/407873900 [01:16<01:39, 2176062.99B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 191806464/407873900 [01:16<01:39, 2173695.07B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 192024576/407873900 [01:16<01:41, 2131409.82B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 192242688/407873900 [01:16<01:43, 2090221.88B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 192472064/407873900 [01:16<01:40, 2142573.16B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 192701440/407873900 [01:16<01:39, 2158139.27B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 192918528/407873900 [01:16<01:41, 2127431.82B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 193143808/407873900 [01:16<01:41, 2114507.53B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 193389568/407873900 [01:17<01:37, 2191319.74B/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 193618944/407873900 [01:17<01:37, 2201026.45B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 193840128/407873900 [01:17<01:37, 2191368.97B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 194061312/407873900 [01:17<01:37, 2187399.70B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 194307072/407873900 [01:17<01:36, 2212348.18B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 194536448/407873900 [01:17<01:35, 2230721.29B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 194759680/407873900 [01:17<01:36, 2213628.14B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 194995200/407873900 [01:17<01:37, 2185448.10B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 195240960/407873900 [01:17<01:35, 2220701.09B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 195499008/407873900 [01:17<01:31, 2324161.47B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 195732480/407873900 [01:18<01:35, 2228704.00B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 195978240/407873900 [01:18<01:33, 2261441.47B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 196224000/407873900 [01:18<01:31, 2307727.54B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 196486144/407873900 [01:18<01:30, 2347280.23B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 196721664/407873900 [01:18<01:31, 2312650.98B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 196977664/407873900 [01:18<01:30, 2342533.90B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 197239808/407873900 [01:18<01:29, 2364449.83B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 197501952/407873900 [01:18<01:26, 2422795.26B/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 197744640/407873900 [01:18<01:27, 2397694.42B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 197985280/407873900 [01:19<01:27, 2395075.37B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 198255616/407873900 [01:19<01:24, 2477146.06B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 198504448/407873900 [01:19<01:24, 2477781.68B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 198763520/407873900 [01:19<01:25, 2455618.80B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 199042048/407873900 [01:19<01:22, 2540986.36B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 199320576/407873900 [01:19<01:20, 2586932.01B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 199582720/407873900 [01:19<01:20, 2588837.90B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 199844864/407873900 [01:19<01:20, 2588644.76B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 200124416/407873900 [01:19<01:18, 2648899.39B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 200418304/407873900 [01:19<01:16, 2722199.40B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 200690688/407873900 [01:20<01:16, 2695812.57B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 200975360/407873900 [01:20<01:16, 2696817.63B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 201286656/407873900 [01:20<01:14, 2782494.72B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 201597952/407873900 [01:20<01:11, 2865607.40B/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 201884672/407873900 [01:20<01:13, 2821761.99B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 202187776/407873900 [01:20<01:11, 2870577.21B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 202482688/407873900 [01:20<01:11, 2884380.47B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 202810368/407873900 [01:20<01:09, 2952613.49B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 203138048/407873900 [01:20<01:07, 3037933.35B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 203450368/407873900 [01:20<01:06, 3062280.01B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 203778048/407873900 [01:21<01:05, 3125442.68B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 204091392/407873900 [01:21<01:06, 3084520.19B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 204400640/407873900 [01:21<01:11, 2827724.43B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 204688384/407873900 [01:21<01:37, 2074816.40B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 205448192/407873900 [01:21<01:01, 3271269.98B/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 205829120/407873900 [01:21<01:06, 3046350.07B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 206173184/407873900 [01:21<01:09, 2886659.16B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 206489600/407873900 [01:22<01:11, 2803094.99B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 206788608/407873900 [01:22<01:12, 2766425.53B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 207078400/407873900 [01:22<01:13, 2730514.88B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 207381504/407873900 [01:22<01:14, 2693554.69B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 207656960/407873900 [01:22<02:14, 1492004.61B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 208299008/407873900 [01:22<01:29, 2222606.56B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 208596992/407873900 [01:23<01:31, 2188990.52B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 208868352/407873900 [01:23<01:31, 2180211.72B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 209123328/407873900 [01:23<01:34, 2107827.51B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 209358848/407873900 [01:23<01:36, 2060599.14B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 209609728/407873900 [01:23<01:38, 2013535.45B/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 209904640/407873900 [01:23<01:32, 2129142.31B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 210166784/407873900 [01:23<01:29, 2214901.98B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 210396160/407873900 [01:23<01:29, 2218484.12B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 210624512/407873900 [01:24<01:31, 2162343.61B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 210844672/407873900 [01:24<01:32, 2125156.62B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 211133440/407873900 [01:24<01:26, 2275602.92B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 211384320/407873900 [01:24<01:23, 2339674.31B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 211620864/407873900 [01:24<01:26, 2256954.55B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 211849216/407873900 [01:24<01:30, 2172203.95B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 212100096/407873900 [01:24<01:28, 2221832.66B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 212378624/407873900 [01:24<01:23, 2330906.55B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 212613120/407873900 [01:24<01:26, 2270036.18B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 212841472/407873900 [01:24<01:28, 2192644.01B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 213099520/407873900 [01:25<01:27, 2222020.54B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 213394432/407873900 [01:25<01:20, 2401855.06B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 213656576/407873900 [01:25<01:19, 2447758.00B/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 213903360/407873900 [01:25<01:22, 2340257.02B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 214138880/407873900 [01:25<01:25, 2256261.42B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 214426624/407873900 [01:25<01:25, 2256567.55B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 214737920/407873900 [01:25<01:19, 2444420.53B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 214984704/407873900 [01:25<01:18, 2448839.05B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 215231488/407873900 [01:25<01:20, 2389733.47B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 215472128/407873900 [01:26<01:22, 2339449.09B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 215753728/407873900 [01:26<01:17, 2467373.74B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 216002560/407873900 [01:26<01:17, 2461449.49B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 216250368/407873900 [01:26<01:19, 2408477.41B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 216492032/407873900 [01:26<01:22, 2324609.65B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 216769536/407873900 [01:26<01:18, 2446878.73B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 217031680/407873900 [01:26<01:16, 2486935.99B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 217281536/407873900 [01:26<01:18, 2438544.72B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 217526272/407873900 [01:26<01:21, 2329658.44B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 217801728/407873900 [01:27<01:18, 2426466.38B/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 218080256/407873900 [01:27<01:15, 2510464.76B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 218333184/407873900 [01:27<01:17, 2446971.24B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 218578944/407873900 [01:27<01:20, 2345341.26B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 218833920/407873900 [01:27<01:21, 2323250.40B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 219145216/407873900 [01:27<01:14, 2531575.36B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 219401216/407873900 [01:27<01:15, 2492924.72B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 219652096/407873900 [01:27<01:18, 2390165.60B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 219892736/407873900 [01:27<01:20, 2335062.59B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 220177408/407873900 [01:28<01:18, 2382711.12B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 220488704/407873900 [01:28<01:14, 2522752.35B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 220741632/407873900 [01:28<01:19, 2346159.46B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 220980224/407873900 [01:28<01:20, 2311307.70B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 221242368/407873900 [01:28<01:18, 2364014.17B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 221488128/407873900 [01:28<01:19, 2347546.02B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 221766656/407873900 [01:28<01:17, 2415981.15B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 222012416/407873900 [01:28<01:17, 2399444.63B/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 222274560/407873900 [01:28<01:16, 2422603.70B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 222520320/407873900 [01:29<01:16, 2410480.62B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 222766080/407873900 [01:29<01:16, 2421066.21B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 223011840/407873900 [01:29<01:17, 2371193.79B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 223290368/407873900 [01:29<01:14, 2474462.06B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 223539200/407873900 [01:29<01:16, 2419676.15B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 223798272/407873900 [01:29<01:16, 2407161.09B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 224044032/407873900 [01:29<01:16, 2416905.54B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 224322560/407873900 [01:29<01:15, 2442472.45B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 224584704/407873900 [01:29<01:14, 2459222.72B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 224831488/407873900 [01:29<01:15, 2420769.24B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 225092608/407873900 [01:30<01:14, 2456416.48B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 225371136/407873900 [01:30<01:12, 2530850.55B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 225625088/407873900 [01:30<01:12, 2505869.47B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 225875968/407873900 [01:30<01:13, 2481750.76B/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 226124800/407873900 [01:30<01:13, 2461883.95B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 226386944/407873900 [01:30<01:12, 2505184.52B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 226637824/407873900 [01:30<01:12, 2494990.36B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 226911232/407873900 [01:30<01:12, 2500922.16B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 227173376/407873900 [01:30<01:12, 2480378.97B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 227468288/407873900 [01:31<01:10, 2576565.69B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 227730432/407873900 [01:31<01:11, 2532088.22B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 227992576/407873900 [01:31<01:10, 2548724.82B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 228254720/407873900 [01:31<01:10, 2549915.89B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 228533248/407873900 [01:31<01:08, 2618052.22B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 228795392/407873900 [01:31<01:08, 2601406.69B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 229057536/407873900 [01:31<01:09, 2584511.28B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 229319680/407873900 [01:31<01:09, 2584640.25B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 229578752/407873900 [01:31<01:09, 2555430.85B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 229834752/407873900 [01:31<01:11, 2505658.84B/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 230138880/407873900 [01:32<01:06, 2659172.76B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 230450176/407873900 [01:32<01:03, 2781736.49B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 230729728/407873900 [01:32<01:05, 2688228.91B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 231000064/407873900 [01:32<01:05, 2683009.47B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 231318528/407873900 [01:32<01:03, 2771129.43B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 231613440/407873900 [01:32<01:02, 2817092.66B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 231908352/407873900 [01:32<01:03, 2780549.66B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 232203264/407873900 [01:32<01:02, 2819625.77B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 232530944/407873900 [01:32<00:59, 2940056.06B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 232825856/407873900 [01:32<00:59, 2927322.91B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 233137152/407873900 [01:33<00:59, 2955871.98B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 233433088/407873900 [01:33<00:59, 2921511.73B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 233808896/407873900 [01:33<00:55, 3114860.83B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 234121216/407873900 [01:33<00:56, 3087827.67B/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 234431488/407873900 [01:33<00:56, 3085777.72B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 234759168/407873900 [01:33<00:56, 3068176.27B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 235152384/407873900 [01:33<00:52, 3296226.48B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 235483136/407873900 [01:33<00:58, 2965032.60B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 235840512/407873900 [01:33<00:55, 3079125.49B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 236184576/407873900 [01:34<00:54, 3145429.51B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 236528640/407873900 [01:34<00:53, 3212000.79B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 236921856/407873900 [01:34<00:50, 3402252.94B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 237265920/407873900 [01:34<00:50, 3400713.85B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 237626368/407873900 [01:34<00:49, 3438755.77B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 238003200/407873900 [01:34<00:48, 3483966.33B/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 238396416/407873900 [01:34<00:47, 3591956.95B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 238789632/407873900 [01:34<00:46, 3665111.09B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 239166464/407873900 [01:34<00:45, 3680129.70B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 239556608/407873900 [01:34<00:44, 3745130.78B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 239936512/407873900 [01:35<00:45, 3726121.30B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 240329728/407873900 [01:35<00:44, 3762184.39B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 240722944/407873900 [01:35<00:44, 3794811.22B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 241102848/407873900 [01:35<00:43, 3795704.38B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 241482752/407873900 [01:35<00:44, 3774227.39B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 241869824/407873900 [01:35<00:43, 3786466.44B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 242248704/407873900 [01:35<00:44, 3761293.48B/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 242639872/407873900 [01:35<00:43, 3781551.40B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 243018752/407873900 [01:35<00:43, 3758946.21B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 243409920/407873900 [01:35<00:43, 3784620.11B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 243788800/407873900 [01:36<00:43, 3785831.78B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 244167680/407873900 [01:36<00:43, 3776228.74B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 244545536/407873900 [01:36<00:47, 3437546.53B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 244895744/407873900 [01:36<00:48, 3365914.35B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 245261312/407873900 [01:36<00:47, 3433336.37B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 245638144/407873900 [01:36<00:46, 3521146.28B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 246014976/407873900 [01:36<00:45, 3591556.14B/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 246393856/407873900 [01:36<00:44, 3648878.39B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 246768640/407873900 [01:36<00:44, 3641054.77B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 247145472/407873900 [01:37<00:43, 3672700.28B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 247522304/407873900 [01:37<00:43, 3698739.07B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 247899136/407873900 [01:37<00:43, 3680380.47B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 248275968/407873900 [01:37<00:43, 3704320.03B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 248647680/407873900 [01:37<00:43, 3700358.19B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 249029632/407873900 [01:37<00:42, 3709617.88B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 249422848/407873900 [01:37<00:42, 3700384.26B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 249799680/407873900 [01:37<00:42, 3717665.47B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 250192896/407873900 [01:37<00:42, 3747452.72B/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 250568704/407873900 [01:37<00:42, 3742125.89B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 250943488/407873900 [01:38<00:41, 3741903.90B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 251323392/407873900 [01:38<00:42, 3685480.91B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 251716608/407873900 [01:38<00:42, 3715121.25B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 252109824/407873900 [01:38<00:41, 3710229.15B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 252486656/407873900 [01:38<00:41, 3717512.17B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 252863488/407873900 [01:38<00:41, 3709666.98B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 253240320/407873900 [01:38<00:41, 3713897.00B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 253617152/407873900 [01:38<00:41, 3708500.03B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 253988864/407873900 [01:39<01:24, 1825581.41B/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 254551040/407873900 [01:39<01:01, 2476254.49B/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 255140864/407873900 [01:39<01:19, 1916255.13B/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 256271360/407873900 [01:40<00:59, 2531694.87B/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 257729536/407873900 [01:40<00:35, 4194163.42B/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 258745344/407873900 [01:40<00:28, 5163089.70B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 259505152/407873900 [01:40<00:39, 3750698.37B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 260335616/407873900 [01:40<00:33, 4449209.24B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 261001216/407873900 [01:40<00:39, 3730542.52B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 261541888/407873900 [01:41<00:41, 3553621.26B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 262011904/407873900 [01:41<00:42, 3422662.12B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 262431744/407873900 [01:41<00:44, 3278105.60B/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 262811648/407873900 [01:41<00:45, 3211352.05B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 263166976/407873900 [01:41<00:45, 3167260.82B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 263506944/407873900 [01:41<00:46, 3098367.66B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 263857152/407873900 [01:41<00:47, 3018354.48B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 264250368/407873900 [01:42<00:44, 3210680.80B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 264583168/407873900 [01:42<00:45, 3159505.03B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 264906752/407873900 [01:42<00:46, 3062621.39B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 265218048/407873900 [01:42<00:49, 2896401.78B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 265511936/407873900 [01:42<00:49, 2881519.93B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 265802752/407873900 [01:42<01:12, 1961602.78B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 266494976/407873900 [01:42<00:48, 2896972.59B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 266839040/407873900 [01:43<01:01, 2293283.12B/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 267122688/407873900 [01:43<01:26, 1634284.60B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 267461632/407873900 [01:43<01:17, 1800406.47B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 267920384/407873900 [01:43<01:01, 2288691.18B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 268217344/407873900 [01:43<01:09, 2002912.27B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 268469248/407873900 [01:44<01:11, 1938705.34B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 268706816/407873900 [01:44<01:14, 1861956.19B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 268968960/407873900 [01:44<01:15, 1851243.07B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 269231104/407873900 [01:44<01:14, 1851461.34B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 269493248/407873900 [01:44<01:14, 1850538.87B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 269755392/407873900 [01:44<01:14, 1848573.46B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 270033920/407873900 [01:44<01:13, 1873515.76B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 270296064/407873900 [01:45<01:13, 1862119.69B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 270574592/407873900 [01:45<01:12, 1890940.06B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 270853120/407873900 [01:45<01:11, 1904033.74B/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 271131648/407873900 [01:45<01:11, 1913708.88B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 271410176/407873900 [01:45<01:06, 2057536.33B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 271623168/407873900 [01:45<01:06, 2037997.32B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 271830016/407873900 [01:45<01:09, 1966663.15B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 272028672/407873900 [01:45<01:11, 1894454.00B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 272278528/407873900 [01:46<01:11, 1902941.16B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 272557056/407873900 [01:46<01:10, 1915275.80B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 272749568/407873900 [01:46<01:18, 1729913.45B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 272950272/407873900 [01:46<01:18, 1714423.42B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 273146880/407873900 [01:46<01:15, 1775250.41B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 273327104/407873900 [01:46<01:18, 1720405.15B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 273622016/407873900 [01:46<01:12, 1857978.97B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 273916928/407873900 [01:46<01:10, 1893185.03B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 274211840/407873900 [01:47<01:08, 1945074.55B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 274490368/407873900 [01:47<01:07, 1972725.29B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 274785280/407873900 [01:47<01:06, 1994744.79B/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 275080192/407873900 [01:47<01:05, 2022281.70B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 275375104/407873900 [01:47<01:04, 2039248.99B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 275653632/407873900 [01:47<01:05, 2006404.56B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 275883008/407873900 [01:47<01:03, 2062772.12B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 276090880/407873900 [01:48<01:10, 1882186.42B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 276281344/407873900 [01:48<01:12, 1812358.44B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 276571136/407873900 [01:48<01:10, 1875489.99B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 276800512/407873900 [01:48<01:08, 1919724.14B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 277029888/407873900 [01:48<01:05, 2005254.53B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 277232640/407873900 [01:48<01:07, 1928342.76B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 277439488/407873900 [01:48<01:06, 1951875.20B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 277668864/407873900 [01:48<01:08, 1906995.63B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 277947392/407873900 [01:49<01:02, 2073481.46B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 278160384/407873900 [01:49<01:02, 2074158.44B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 278369280/407873900 [01:49<01:05, 1992374.99B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 278569984/407873900 [01:49<01:05, 1962389.65B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 278832128/407873900 [01:49<01:05, 1979886.03B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 279110656/407873900 [01:49<00:58, 2187393.67B/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 279331840/407873900 [01:49<01:01, 2083426.57B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 279542784/407873900 [01:49<01:03, 2012411.86B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 279745536/407873900 [01:49<01:04, 2000674.77B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 279995392/407873900 [01:50<01:01, 2074225.03B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 280220672/407873900 [01:50<01:00, 2123867.63B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 280434688/407873900 [01:50<01:02, 2054152.60B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 280641536/407873900 [01:50<01:03, 2008292.96B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 280896512/407873900 [01:50<01:03, 1996337.42B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 281175040/407873900 [01:50<00:59, 2144859.09B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 281391104/407873900 [01:50<00:58, 2145059.07B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 281606144/407873900 [01:50<01:01, 2069200.27B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 281814016/407873900 [01:50<01:01, 2048648.84B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 282076160/407873900 [01:51<00:59, 2130667.69B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 282305536/407873900 [01:51<00:58, 2141589.56B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 282520576/407873900 [01:51<01:00, 2071028.18B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 282731520/407873900 [01:51<01:01, 2022306.33B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 282993664/407873900 [01:51<00:57, 2167458.13B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 283211776/407873900 [01:51<01:02, 1985848.65B/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 283413504/407873900 [01:51<01:04, 1928067.58B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 283681792/407873900 [01:51<00:59, 2085057.09B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 283892736/407873900 [01:51<01:04, 1927538.51B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 284089344/407873900 [01:52<01:06, 1857637.65B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 284369920/407873900 [01:52<00:58, 2101117.45B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 284615680/407873900 [01:52<00:56, 2194970.31B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 284838912/407873900 [01:52<00:57, 2149759.65B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 285058048/407873900 [01:52<00:58, 2086913.29B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 285369344/407873900 [01:52<00:51, 2358830.05B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 285631488/407873900 [01:52<00:50, 2405806.75B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 285874176/407873900 [01:52<00:52, 2327135.10B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 286108672/407873900 [01:52<00:53, 2257604.00B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 286401536/407873900 [01:53<00:49, 2435071.53B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 286680064/407873900 [01:53<00:47, 2528695.69B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 286935040/407873900 [01:53<00:50, 2417073.73B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 287178752/407873900 [01:53<00:50, 2372772.44B/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 287499264/407873900 [01:53<00:46, 2581147.54B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 287777792/407873900 [01:53<00:45, 2613749.49B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 288040960/407873900 [01:53<00:46, 2558984.09B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 288297984/407873900 [01:53<00:47, 2544098.72B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 288629760/407873900 [01:53<00:43, 2716953.99B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 288908288/407873900 [01:53<00:43, 2722319.86B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 289181696/407873900 [01:54<00:45, 2626984.14B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 289448960/407873900 [01:54<00:52, 2252382.92B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 289711104/407873900 [01:54<01:16, 1535051.07B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 290481152/407873900 [01:54<00:43, 2725784.17B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 290842624/407873900 [01:54<00:48, 2408854.58B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 291150848/407873900 [01:54<00:49, 2366697.68B/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 291433472/407873900 [01:55<00:50, 2304993.66B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 291695616/407873900 [01:55<00:51, 2244169.69B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 291955712/407873900 [01:55<00:51, 2233736.99B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 292250624/407873900 [01:55<00:48, 2374626.72B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 292529152/407873900 [01:55<00:46, 2474227.79B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 292788224/407873900 [01:55<00:47, 2421000.59B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 293038080/407873900 [01:55<00:49, 2334506.13B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 293348352/407873900 [01:55<00:48, 2350305.58B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 293692416/407873900 [01:56<00:43, 2597943.31B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 293987328/407873900 [01:56<00:42, 2670311.75B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 294258688/407873900 [01:56<00:44, 2543421.20B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 294516736/407873900 [01:56<00:45, 2517376.53B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 294840320/407873900 [01:56<00:41, 2716227.32B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 295115776/407873900 [01:56<00:43, 2594022.28B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 295378944/407873900 [01:56<00:45, 2468476.24B/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 295628800/407873900 [01:57<01:21, 1382892.08B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 296363008/407873900 [01:57<00:46, 2405480.68B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 296698880/407873900 [01:57<00:49, 2265753.94B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 296992768/407873900 [01:57<00:50, 2193256.14B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 297257984/407873900 [01:57<00:51, 2142088.53B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 297503744/407873900 [01:57<00:52, 2082577.19B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 297733120/407873900 [01:57<00:54, 2023169.77B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 298017792/407873900 [01:58<00:54, 2000038.70B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 298329088/407873900 [01:58<00:53, 2061261.35B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 298640384/407873900 [01:58<00:51, 2105346.62B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 298951680/407873900 [01:58<00:51, 2134928.68B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 299262976/407873900 [01:58<00:50, 2153137.22B/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 299590656/407873900 [01:58<00:49, 2204597.63B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 299918336/407873900 [01:58<00:48, 2238972.19B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 300229632/407873900 [01:59<00:46, 2331587.33B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 300524544/407873900 [01:59<00:43, 2458157.64B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 300774400/407873900 [01:59<00:45, 2353035.33B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 301012992/407873900 [01:59<00:46, 2276474.15B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 301242368/407873900 [01:59<00:48, 2206426.84B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 301556736/407873900 [01:59<00:48, 2205672.00B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 301884416/407873900 [01:59<00:47, 2244245.57B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 302212096/407873900 [01:59<00:42, 2459798.03B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 302461952/407873900 [01:59<00:43, 2447994.31B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 302709760/407873900 [02:00<00:44, 2374345.53B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 302949376/407873900 [02:00<00:45, 2283800.48B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 303227904/407873900 [02:00<00:44, 2332148.69B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 303506432/407873900 [02:00<00:42, 2438981.65B/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 303752192/407873900 [02:00<00:43, 2410509.15B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 303994880/407873900 [02:00<00:44, 2325992.94B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 304243712/407873900 [02:00<00:46, 2244729.41B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 304571392/407873900 [02:00<00:42, 2419636.10B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 304833536/407873900 [02:00<00:41, 2456633.43B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 305080320/407873900 [02:01<00:43, 2366082.86B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 305317888/407873900 [02:01<00:44, 2323023.24B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 305570816/407873900 [02:01<00:43, 2325185.54B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 305804288/407873900 [02:01<00:45, 2255105.95B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 306045952/407873900 [02:01<00:44, 2293061.79B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 306276352/407873900 [02:01<00:45, 2244871.91B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 306521088/407873900 [02:01<00:44, 2263802.05B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 306748416/407873900 [02:01<00:50, 2005148.51B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 307028992/407873900 [02:01<00:48, 2072273.97B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 307356672/407873900 [02:02<00:42, 2365427.77B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 307599360/407873900 [02:02<00:42, 2373016.45B/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 307841024/407873900 [02:02<00:44, 2237867.70B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 308069376/407873900 [02:02<00:46, 2146440.83B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 308324352/407873900 [02:02<00:44, 2254428.27B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 308553728/407873900 [02:02<00:48, 2065270.30B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 308765696/407873900 [02:02<00:48, 2045429.12B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 309076992/407873900 [02:02<00:44, 2228582.48B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 309339136/407873900 [02:02<00:42, 2317959.23B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 309584896/407873900 [02:03<00:42, 2338492.03B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 309820416/407873900 [02:03<00:42, 2281914.97B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 310092800/407873900 [02:03<00:42, 2302133.35B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 310371328/407873900 [02:03<00:40, 2425684.98B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 310617088/407873900 [02:03<00:40, 2422245.75B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 310860800/407873900 [02:03<00:41, 2358649.02B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 311124992/407873900 [02:03<00:42, 2271237.62B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 311436288/407873900 [02:03<00:38, 2478913.83B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 311698432/407873900 [02:03<00:38, 2483852.64B/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 311949312/407873900 [02:04<00:39, 2401064.49B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 312190976/407873900 [02:04<00:41, 2320075.73B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 312484864/407873900 [02:04<00:38, 2447612.82B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 312763392/407873900 [02:04<00:38, 2495460.36B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 313014272/407873900 [02:04<00:38, 2442421.12B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 313260032/407873900 [02:04<00:39, 2394723.48B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 313533440/407873900 [02:04<00:38, 2430643.67B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 313811968/407873900 [02:04<00:37, 2499906.18B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 314062848/407873900 [02:04<00:37, 2494515.76B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 314312704/407873900 [02:05<00:38, 2399032.28B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 314553344/407873900 [02:05<00:39, 2390827.92B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 314792960/407873900 [02:05<00:41, 2225869.79B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 315018240/407873900 [02:05<00:44, 2094484.20B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 315302912/407873900 [02:05<00:40, 2266457.38B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 315533312/407873900 [02:05<00:42, 2158685.30B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 315778048/407873900 [02:05<00:42, 2159873.01B/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 315996160/407873900 [02:05<00:44, 2059898.67B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 316204032/407873900 [02:05<00:44, 2058431.23B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 316564480/407873900 [02:06<00:40, 2264644.18B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 316941312/407873900 [02:06<00:37, 2397414.57B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 317318144/407873900 [02:06<00:33, 2728371.27B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 317594624/407873900 [02:06<00:33, 2687328.72B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 317865984/407873900 [02:06<00:35, 2563269.97B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 318124032/407873900 [02:06<00:38, 2312498.82B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 318415872/407873900 [02:06<00:36, 2419771.88B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 318662656/407873900 [02:06<00:40, 2225928.92B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 318891008/407873900 [02:07<00:39, 2236219.60B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 319118336/407873900 [02:07<00:40, 2216238.62B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 319431680/407873900 [02:07<00:36, 2441723.59B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 319679488/407873900 [02:07<00:36, 2405333.92B/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 319939584/407873900 [02:07<00:36, 2434732.94B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 320250880/407873900 [02:07<00:33, 2608880.10B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 320514048/407873900 [02:07<00:33, 2576925.20B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 320773120/407873900 [02:07<00:34, 2539312.12B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 321102848/407873900 [02:07<00:31, 2749295.17B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 321414144/407873900 [02:07<00:30, 2849298.43B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 321725440/407873900 [02:08<00:29, 2898855.38B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 322118656/407873900 [02:08<00:26, 3177497.53B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 322447360/407873900 [02:08<00:26, 3208330.58B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 322774016/407873900 [02:08<00:26, 3211437.58B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 323167232/407873900 [02:08<00:25, 3305981.80B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 323527680/407873900 [02:08<00:25, 3364859.64B/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 323904512/407873900 [02:08<00:24, 3475283.90B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 324264960/407873900 [02:08<00:23, 3502120.13B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 324658176/407873900 [02:08<00:23, 3595042.89B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 325035008/407873900 [02:09<00:22, 3620883.87B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 325428224/407873900 [02:09<00:22, 3685151.20B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 325796864/407873900 [02:09<00:22, 3674018.48B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 326181888/407873900 [02:09<00:22, 3678235.42B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 326550528/407873900 [02:09<00:23, 3521119.52B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 326904832/407873900 [02:09<00:23, 3417249.98B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 327247872/407873900 [02:09<00:23, 3379247.35B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 327623680/407873900 [02:09<00:23, 3448470.39B/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 328000512/407873900 [02:09<00:22, 3538926.07B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 328355840/407873900 [02:09<00:22, 3471036.39B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 328704000/407873900 [02:10<00:23, 3382582.61B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 329081856/407873900 [02:10<00:22, 3475354.03B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 329458688/407873900 [02:10<00:22, 3512003.15B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 329851904/407873900 [02:10<00:21, 3625561.62B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 330215424/407873900 [02:10<00:21, 3627930.44B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 330605568/407873900 [02:10<00:20, 3683595.14B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 330992640/407873900 [02:10<00:20, 3738622.02B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 331367424/407873900 [02:10<00:20, 3718629.33B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 331752448/407873900 [02:10<00:20, 3730317.05B/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 332145664/407873900 [02:10<00:20, 3713107.54B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 332523520/407873900 [02:11<00:20, 3731765.46B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 332899328/407873900 [02:11<00:20, 3722814.83B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 333272064/407873900 [02:11<00:20, 3722291.86B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 333644800/407873900 [02:11<00:19, 3711956.88B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 334016512/407873900 [02:11<00:19, 3706535.83B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 334390272/407873900 [02:11<00:19, 3679743.17B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 334767104/407873900 [02:11<00:19, 3694866.08B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 335143936/407873900 [02:11<00:19, 3703577.79B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 335520768/407873900 [02:11<00:19, 3674858.43B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 335897600/407873900 [02:12<00:19, 3694235.74B/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 336290816/407873900 [02:12<00:19, 3701363.39B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 336667648/407873900 [02:12<00:19, 3703837.31B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 337044480/407873900 [02:12<00:19, 3692376.07B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 337421312/407873900 [02:12<00:19, 3680426.64B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 337798144/407873900 [02:12<00:19, 3684273.36B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 338174976/407873900 [02:12<00:18, 3683380.24B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 338551808/407873900 [02:12<00:18, 3675824.14B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 338919424/407873900 [02:13<00:38, 1770427.63B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 339289088/407873900 [02:13<00:42, 1627568.91B/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 339993600/407873900 [02:13<00:29, 2265053.80B/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 341255168/407873900 [02:13<00:16, 3947282.57B/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 341801984/407873900 [02:14<00:28, 2314149.67B/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 343499776/407873900 [02:14<00:15, 4224122.36B/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 344294400/407873900 [02:14<00:18, 3362364.07B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 344914944/407873900 [02:14<00:19, 3220036.24B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 345434112/407873900 [02:15<00:25, 2489523.22B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 345891840/407873900 [02:15<00:22, 2723267.28B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 346300416/407873900 [02:15<00:24, 2507265.16B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 346644480/407873900 [02:15<00:27, 2190749.67B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 346929152/407873900 [02:16<00:29, 2089647.20B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 347181056/407873900 [02:16<00:28, 2124317.01B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 347425792/407873900 [02:16<00:29, 2082568.09B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 347656192/407873900 [02:16<00:30, 1973328.35B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 347867136/407873900 [02:16<00:34, 1756917.94B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 348053504/407873900 [02:16<00:45, 1324345.10B/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 348595200/407873900 [02:17<00:30, 1936308.19B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 348824576/407873900 [02:17<00:31, 1851597.24B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 349070336/407873900 [02:17<00:33, 1780874.05B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 349299712/407873900 [02:17<00:33, 1765468.97B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 349545472/407873900 [02:17<00:33, 1748997.26B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 349774848/407873900 [02:17<00:31, 1864249.90B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 349969408/407873900 [02:17<00:33, 1720401.00B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 350147584/407873900 [02:17<00:35, 1644039.65B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 350316544/407873900 [02:18<00:36, 1581761.22B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 350544896/407873900 [02:18<00:34, 1647264.89B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 350807040/407873900 [02:18<00:33, 1710169.68B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 351069184/407873900 [02:18<00:32, 1754976.88B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 351265792/407873900 [02:18<00:45, 1252240.18B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 351691776/407873900 [02:18<00:30, 1813273.86B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 351913984/407873900 [02:19<00:33, 1688801.26B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 352111616/407873900 [02:19<00:35, 1592911.87B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 352290816/407873900 [02:19<00:36, 1507043.78B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 352454656/407873900 [02:19<00:37, 1488782.27B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 352612352/407873900 [02:19<00:38, 1417422.38B/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 352773120/407873900 [02:19<00:38, 1431634.10B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 352936960/407873900 [02:19<00:38, 1438362.65B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 353084416/407873900 [02:19<00:38, 1416024.77B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 353248256/407873900 [02:19<00:37, 1456457.34B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 353395712/407873900 [02:20<00:38, 1408957.37B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 353543168/407873900 [02:20<00:39, 1373909.55B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 353707008/407873900 [02:20<00:38, 1411787.24B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 353849344/407873900 [02:20<00:38, 1387905.10B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 353988608/407873900 [02:20<00:56, 957490.89B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 354231296/407873900 [02:20<00:42, 1260146.96B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 354380800/407873900 [02:20<00:46, 1162867.19B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 354513920/407873900 [02:21<00:46, 1153865.74B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 354640896/407873900 [02:21<00:47, 1115233.38B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 354760704/407873900 [02:21<00:49, 1068765.65B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 354873344/407873900 [02:21<00:49, 1076418.21B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 354984960/407873900 [02:21<00:55, 957897.58B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355116032/407873900 [02:21<00:52, 1001364.74B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355263488/407873900 [02:21<00:49, 1069528.18B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355378176/407873900 [02:21<00:49, 1067290.88B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 355487744/407873900 [02:22<00:54, 957221.42B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355640320/407873900 [02:22<00:47, 1092458.20B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355753984/407873900 [02:22<00:48, 1068925.71B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 355864576/407873900 [02:22<00:52, 983284.70B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 356017152/407873900 [02:22<00:52, 986092.10B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 356197376/407873900 [02:22<00:47, 1077506.74B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 356377600/407873900 [02:22<00:45, 1139179.08B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 356557824/407873900 [02:22<00:43, 1182552.24B/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 356738048/407873900 [02:23<00:42, 1214412.39B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 356910080/407873900 [02:23<00:38, 1333745.11B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 357091328/407873900 [02:23<00:34, 1454422.02B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357241856/407873900 [02:23<01:22, 615595.67B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357377024/407873900 [02:24<01:11, 707120.59B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357508096/407873900 [02:24<01:06, 754508.87B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357639168/407873900 [02:24<01:02, 800439.41B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357770240/407873900 [02:24<01:00, 834192.99B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 357917696/407873900 [02:24<00:56, 891496.75B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 358065152/407873900 [02:24<00:53, 934791.53B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 358228992/407873900 [02:24<00:50, 990601.59B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 358376448/407873900 [02:25<00:48, 1014613.56B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 358540288/407873900 [02:25<00:46, 1056004.57B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 358704128/407873900 [02:25<00:46, 1065009.45B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 358867968/407873900 [02:25<00:43, 1120743.71B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 359031808/407873900 [02:25<00:43, 1134570.92B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 359195648/407873900 [02:25<00:43, 1123063.00B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 359375872/407873900 [02:25<00:41, 1170810.05B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 359525376/407873900 [02:26<00:38, 1245675.65B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 359703552/407873900 [02:26<00:38, 1251675.55B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 359883776/407873900 [02:26<00:38, 1261785.01B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 360047616/407873900 [02:26<00:35, 1341578.84B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360184832/407873900 [02:26<00:37, 1287562.58B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360315904/407873900 [02:26<00:44, 1073785.07B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360429568/407873900 [02:26<00:44, 1074117.72B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360541184/407873900 [02:26<00:43, 1079790.44B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360686592/407873900 [02:27<00:41, 1144307.59B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360804352/407873900 [02:27<00:41, 1136613.73B/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 360920064/407873900 [02:27<00:41, 1122518.73B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 361047040/407873900 [02:27<00:41, 1140629.27B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 361178112/407873900 [02:27<00:42, 1088416.45B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361292800/407873900 [02:27<00:44, 1045196.36B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361423872/407873900 [02:27<00:41, 1111498.86B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361536512/407873900 [02:27<00:45, 1012981.50B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361669632/407873900 [02:27<00:45, 1011284.28B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361849856/407873900 [02:28<00:42, 1081937.24B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 361958400/407873900 [02:28<00:43, 1054211.88B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 362063872/407873900 [02:28<00:45, 1000641.33B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 362243072/407873900 [02:28<00:42, 1062541.12B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 362390528/407873900 [02:28<00:40, 1120142.38B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 362554368/407873900 [02:28<00:36, 1243421.68B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 362681344/407873900 [02:28<00:38, 1178142.06B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 362801152/407873900 [02:28<00:39, 1130005.95B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 362963968/407873900 [02:29<00:36, 1222013.66B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 363127808/407873900 [02:29<00:34, 1284608.54B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 363258880/407873900 [02:29<00:34, 1290472.40B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 363388928/407873900 [02:29<00:36, 1231436.46B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 363521024/407873900 [02:29<00:36, 1216500.33B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 363668480/407873900 [02:29<00:35, 1261925.80B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 363815936/407873900 [02:29<00:33, 1308634.37B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 363948032/407873900 [02:29<00:35, 1227850.36B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 364078080/407873900 [02:29<00:35, 1230144.47B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 364225536/407873900 [02:30<00:34, 1279719.84B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 364389376/407873900 [02:30<00:31, 1360803.53B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 364526592/407873900 [02:30<00:34, 1245207.10B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 364653568/407873900 [02:30<00:34, 1250614.50B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 364815360/407873900 [02:30<00:34, 1257739.07B/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 364979200/407873900 [02:30<00:32, 1317768.02B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 365126656/407873900 [02:30<00:31, 1359296.57B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 365263872/407873900 [02:30<00:32, 1303732.64B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 365405184/407873900 [02:30<00:33, 1254262.00B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 365569024/407873900 [02:31<00:31, 1350817.45B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 365716480/407873900 [02:31<00:30, 1367126.31B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 365863936/407873900 [02:31<00:30, 1364965.43B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 366002176/407873900 [02:31<00:31, 1311004.29B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 366175232/407873900 [02:31<00:31, 1318180.97B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 366339072/407873900 [02:31<00:30, 1383231.17B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 366502912/407873900 [02:31<00:29, 1426488.77B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 366650368/407873900 [02:31<00:28, 1421644.35B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 366793728/407873900 [02:31<00:29, 1369717.54B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 366978048/407873900 [02:32<00:29, 1370296.24B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 367174656/407873900 [02:32<00:28, 1438522.80B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 367338496/407873900 [02:32<00:27, 1479816.21B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 367518720/407873900 [02:32<00:26, 1511234.43B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 367670272/407873900 [02:32<00:30, 1315284.23B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 367846400/407873900 [02:32<00:28, 1414456.75B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 367993856/407873900 [02:32<00:28, 1403195.66B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 368137216/407873900 [02:32<00:29, 1368897.66B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 368305152/407873900 [02:33<00:28, 1409800.28B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 368448512/407873900 [02:33<00:28, 1392664.03B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 368600064/407873900 [02:33<00:29, 1348944.89B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 368736256/407873900 [02:33<00:29, 1308863.13B/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 368927744/407873900 [02:33<00:27, 1412814.48B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 369145856/407873900 [02:33<00:23, 1622054.43B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 369310720/407873900 [02:33<00:25, 1524924.85B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 369468416/407873900 [02:33<00:26, 1434673.22B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 369714176/407873900 [02:33<00:23, 1597032.23B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 369874944/407873900 [02:34<00:23, 1590991.02B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 370034688/407873900 [02:34<00:39, 967762.77B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 370160640/407873900 [02:34<00:37, 1012576.57B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 370418688/407873900 [02:34<00:34, 1093512.85B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 370542592/407873900 [02:34<00:35, 1040928.84B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 370844672/407873900 [02:34<00:25, 1426255.64B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371007488/407873900 [02:35<00:28, 1312909.49B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371153920/407873900 [02:35<00:29, 1226807.36B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371287040/407873900 [02:35<00:31, 1152705.18B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371408896/407873900 [02:35<00:32, 1122005.12B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371532800/407873900 [02:35<00:34, 1061792.60B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371663872/407873900 [02:35<00:33, 1096852.26B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371776512/407873900 [02:35<00:33, 1086239.50B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 371893248/407873900 [02:35<00:33, 1065000.39B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 372007936/407873900 [02:36<00:33, 1086409.24B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372139008/407873900 [02:36<00:34, 1032473.37B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372286464/407873900 [02:36<00:31, 1142084.49B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372404224/407873900 [02:36<00:32, 1098332.60B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372532224/407873900 [02:36<00:31, 1113617.38B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372646912/407873900 [02:36<00:31, 1116962.11B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372777984/407873900 [02:36<00:31, 1097270.91B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 372925440/407873900 [02:36<00:29, 1178944.31B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 373045248/407873900 [02:36<00:29, 1181504.46B/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373165056/407873900 [02:37<00:29, 1182360.40B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373283840/407873900 [02:37<00:29, 1158724.12B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373416960/407873900 [02:37<00:30, 1125900.93B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373564416/407873900 [02:37<00:28, 1191698.27B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373684224/407873900 [02:37<00:29, 1159269.86B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373800960/407873900 [02:37<00:29, 1160632.03B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373917696/407873900 [02:37<00:33, 1022997.15B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 374023168/407873900 [02:37<00:34, 969359.84B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 374187008/407873900 [02:38<00:32, 1037835.91B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374367232/407873900 [02:38<00:30, 1101599.39B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374531072/407873900 [02:38<00:29, 1118052.11B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374694912/407873900 [02:38<00:29, 1137976.14B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374858752/407873900 [02:38<00:28, 1143071.18B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375022592/407873900 [02:38<00:28, 1153863.24B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375186432/407873900 [02:38<00:28, 1161893.27B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 375350272/407873900 [02:39<00:27, 1164091.85B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 375514112/407873900 [02:39<00:26, 1202231.01B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 375677952/407873900 [02:39<00:27, 1163700.14B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 375841792/407873900 [02:39<00:27, 1180376.81B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376005632/407873900 [02:39<00:26, 1182650.55B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376169472/407873900 [02:39<00:26, 1181800.15B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376333312/407873900 [02:39<00:26, 1182001.30B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376497152/407873900 [02:39<00:26, 1181337.02B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376660992/407873900 [02:40<00:26, 1184095.17B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376824832/407873900 [02:40<00:25, 1200720.88B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376988672/407873900 [02:40<00:26, 1179213.22B/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377168896/407873900 [02:40<00:25, 1204372.71B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377332736/407873900 [02:40<00:25, 1206837.39B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377496576/407873900 [02:40<00:24, 1225394.55B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 377660416/407873900 [02:40<00:23, 1313043.48B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 377794560/407873900 [02:41<00:22, 1310423.80B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 377927680/407873900 [02:41<00:24, 1244974.34B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 378053632/407873900 [02:41<00:25, 1189712.03B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 378201088/407873900 [02:41<00:25, 1148529.98B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 378364928/407873900 [02:41<00:24, 1217467.66B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 378545152/407873900 [02:41<00:24, 1189534.95B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 378725376/407873900 [02:41<00:23, 1224006.92B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 378905600/407873900 [02:41<00:23, 1251043.63B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379085824/407873900 [02:42<00:22, 1298694.82B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379266048/407873900 [02:42<00:21, 1328713.33B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379462656/407873900 [02:42<00:21, 1334048.87B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379659264/407873900 [02:42<00:21, 1336044.32B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 379855872/407873900 [02:42<00:21, 1329742.48B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380052480/407873900 [02:42<00:20, 1373017.41B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380249088/407873900 [02:42<00:18, 1503041.06B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380429312/407873900 [02:42<00:17, 1531067.53B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380585984/407873900 [02:43<00:18, 1484536.80B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380736512/407873900 [02:43<00:19, 1418101.93B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 380904448/407873900 [02:43<00:18, 1447947.34B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 381101056/407873900 [02:43<00:17, 1541246.83B/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 381314048/407873900 [02:43<00:16, 1641625.41B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 381479936/407873900 [02:43<00:16, 1636613.61B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 381644800/407873900 [02:43<00:16, 1570347.92B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 381821952/407873900 [02:43<00:16, 1575588.58B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 382051328/407873900 [02:44<00:15, 1680876.19B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 382280704/407873900 [02:44<00:13, 1833452.90B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 382466048/407873900 [02:44<00:13, 1830656.03B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 382650368/407873900 [02:44<00:14, 1754841.28B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 382854144/407873900 [02:44<00:14, 1762587.17B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383116288/407873900 [02:44<00:12, 1911324.13B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383362048/407873900 [02:44<00:11, 2051886.46B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383568896/407873900 [02:44<00:12, 2024031.51B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383772672/407873900 [02:44<00:12, 1952119.43B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 384017408/407873900 [02:45<00:11, 1990533.15B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 384312320/407873900 [02:45<00:10, 2179744.47B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 384590848/407873900 [02:45<00:10, 2281296.15B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 384820224/407873900 [02:45<00:10, 2239925.86B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 385044480/407873900 [02:45<00:10, 2201203.30B/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 385328128/407873900 [02:45<00:09, 2270064.14B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 385655808/407873900 [02:45<00:09, 2439026.81B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 385967104/407873900 [02:45<00:08, 2610752.53B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 386229248/407873900 [02:45<00:08, 2525126.63B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 386491392/407873900 [02:45<00:08, 2514870.07B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 386819072/407873900 [02:46<00:08, 2611347.13B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 387163136/407873900 [02:46<00:07, 2826510.72B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 387447808/407873900 [02:46<00:07, 2777860.14B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 387752960/407873900 [02:46<00:07, 2794639.51B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 388097024/407873900 [02:46<00:06, 2956541.38B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 388441088/407873900 [02:46<00:06, 3086113.12B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 388751360/407873900 [02:46<00:06, 3080829.92B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 389096448/407873900 [02:46<00:06, 3124164.62B/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 389444608/407873900 [02:46<00:05, 3227693.50B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 389800960/407873900 [02:47<00:05, 3292150.23B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 390194176/407873900 [02:47<00:05, 3442612.24B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 390554624/407873900 [02:47<00:04, 3488731.06B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 390947840/407873900 [02:47<00:04, 3576905.74B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 391341056/407873900 [02:47<00:04, 3654563.77B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 391734272/407873900 [02:47<00:04, 3706275.79B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 392116224/407873900 [02:47<00:04, 3739398.74B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 392494080/407873900 [02:47<00:04, 3750973.76B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 392881152/407873900 [02:47<00:03, 3773732.74B/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 393274368/407873900 [02:47<00:03, 3788395.65B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 393667584/407873900 [02:48<00:03, 3800711.31B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 394060800/407873900 [02:48<00:03, 3789531.84B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 394454016/407873900 [02:48<00:03, 3784569.94B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 394847232/407873900 [02:48<00:03, 3789109.73B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395227136/407873900 [02:48<00:03, 3760621.68B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395617280/407873900 [02:48<00:03, 3760788.74B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396010496/407873900 [02:48<00:03, 3763467.58B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396397568/407873900 [02:48<00:03, 3794771.66B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396780544/407873900 [02:48<00:02, 3763893.15B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 397167616/407873900 [02:48<00:02, 3795277.35B/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 397547520/407873900 [02:49<00:02, 3794173.13B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 397943808/407873900 [02:49<00:02, 3761200.22B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 398325760/407873900 [02:49<00:02, 3777949.66B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 398704640/407873900 [02:49<00:02, 3762555.60B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 399081472/407873900 [02:49<00:02, 3736107.13B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 399467520/407873900 [02:49<00:02, 3753419.00B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 399844352/407873900 [02:49<00:02, 3746606.52B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 400221184/407873900 [02:49<00:02, 3728679.67B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 400598016/407873900 [02:49<00:01, 3721649.51B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 400991232/407873900 [02:50<00:01, 3766406.48B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 401368064/407873900 [02:50<00:01, 3749964.77B/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 401744896/407873900 [02:50<00:01, 3749449.79B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 402121728/407873900 [02:50<00:01, 3733101.22B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 402498560/407873900 [02:50<00:01, 3741350.13B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 402891776/407873900 [02:50<00:01, 3762234.91B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 403268608/407873900 [02:50<00:01, 3721751.46B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 403641344/407873900 [02:51<00:02, 2041242.31B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 403932160/407873900 [02:51<00:02, 1738574.25B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 404415488/407873900 [02:51<00:01, 2175242.08B/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 405775360/407873900 [02:51<00:00, 4312257.19B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 406619136/407873900 [02:51<00:00, 5191770.74B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 407276544/407873900 [02:51<00:00, 3438526.38B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 407873900/407873900 [02:51<00:00, 2372054.05B/s]
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
         ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ~~~~^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 12:00:41 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:00:41 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
         ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ~~~~^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 12:01:29 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:01:29 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
         ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ~~~~^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 12:01:44 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:01:44 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
         ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ~~~~^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 12:02:01 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:02:01 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1484, in load
    with _open_file_like(f, "rb") as opened_file:
         ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ~~~~^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 12:02:17 2025 ===


=== RUN START epoch 1 time Wed Oct 15 12:06:04 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 12:06:20 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:06:20 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 12:06:37 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:06:37 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 12:06:55 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:06:55 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 12:07:13 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:07:13 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 12:07:29 2025 ===


=== RUN START epoch 1 time Wed Oct 15 12:51:59 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 1 rc 1 time Wed Oct 15 12:51:59 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:51:59 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 2 rc 1 time Wed Oct 15 12:51:59 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:51:59 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 3 rc 1 time Wed Oct 15 12:51:59 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:51:59 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 4 rc 1 time Wed Oct 15 12:51:59 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:51:59 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 5 rc 1 time Wed Oct 15 12:51:59 2025 ===


=== RUN START epoch 1 time Wed Oct 15 12:53:23 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 1 rc 1 time Wed Oct 15 12:53:23 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:53:23 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 2 rc 1 time Wed Oct 15 12:53:23 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:53:23 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 3 rc 1 time Wed Oct 15 12:53:23 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:53:23 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 4 rc 1 time Wed Oct 15 12:53:24 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:53:24 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 5 rc 1 time Wed Oct 15 12:53:24 2025 ===


=== RUN START epoch 1 time Wed Oct 15 12:57:12 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 1 rc 1 time Wed Oct 15 12:57:12 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:57:12 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 2 rc 1 time Wed Oct 15 12:57:13 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:57:13 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 3 rc 1 time Wed Oct 15 12:57:13 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:57:13 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 4 rc 1 time Wed Oct 15 12:57:13 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:57:13 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 5 rc 1 time Wed Oct 15 12:57:13 2025 ===


=== RUN START epoch 1 time Wed Oct 15 12:57:32 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 12:58:27 2025 ===


=== RUN START epoch 2 time Wed Oct 15 12:58:27 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 12:58:46 2025 ===


=== RUN START epoch 3 time Wed Oct 15 12:58:46 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 12:59:03 2025 ===


=== RUN START epoch 4 time Wed Oct 15 12:59:03 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 12:59:21 2025 ===


=== RUN START epoch 5 time Wed Oct 15 12:59:21 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 12:59:42 2025 ===


=== RUN START epoch 1 time Wed Oct 15 13:06:40 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 1 rc 1 time Wed Oct 15 13:06:40 2025 ===


=== RUN START epoch 2 time Wed Oct 15 13:06:40 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 2 rc 1 time Wed Oct 15 13:06:40 2025 ===


=== RUN START epoch 3 time Wed Oct 15 13:06:40 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 3 rc 1 time Wed Oct 15 13:06:40 2025 ===


=== RUN START epoch 4 time Wed Oct 15 13:06:40 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 4 rc 1 time Wed Oct 15 13:06:41 2025 ===


=== RUN START epoch 5 time Wed Oct 15 13:06:41 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

=== RUN END epoch 5 rc 1 time Wed Oct 15 13:06:41 2025 ===


=== RUN START epoch 1 time Wed Oct 15 13:07:37 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 1 rc 1 time Wed Oct 15 13:07:37 2025 ===


=== RUN START epoch 2 time Wed Oct 15 13:07:37 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 2 rc 1 time Wed Oct 15 13:07:37 2025 ===


=== RUN START epoch 3 time Wed Oct 15 13:07:37 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 3 rc 1 time Wed Oct 15 13:07:38 2025 ===


=== RUN START epoch 4 time Wed Oct 15 13:07:38 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 4 rc 1 time Wed Oct 15 13:07:38 2025 ===


=== RUN START epoch 5 time Wed Oct 15 13:07:38 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 5 rc 1 time Wed Oct 15 13:07:39 2025 ===


=== RUN START epoch 1 time Wed Oct 15 13:13:22 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 1 rc 0 time Wed Oct 15 13:13:38 2025 ===


=== RUN START epoch 2 time Wed Oct 15 13:13:38 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 2 rc 0 time Wed Oct 15 13:13:54 2025 ===


=== RUN START epoch 3 time Wed Oct 15 13:13:54 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 3 rc 0 time Wed Oct 15 13:14:14 2025 ===


=== RUN START epoch 4 time Wed Oct 15 13:14:14 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 4 rc 0 time Wed Oct 15 13:14:32 2025 ===


=== RUN START epoch 5 time Wed Oct 15 13:14:32 2025 ===
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/saifm/seqtrack_env/lib/python3.13/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:320: UserWarning: Overwriting vit_base_patch16 in registry with lib.models.seqtrackv2.vit.vit_base_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:339: UserWarning: Overwriting vit_large_patch16 in registry with lib.models.seqtrackv2.vit.vit_large_patch16. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/vit.py:357: UserWarning: Overwriting vit_huge_patch14 in registry with lib.models.seqtrackv2.vit.vit_huge_patch14. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  @register_model
Using device: cpu
local_rank: -1
script_name: seqtrackv2.py  config_name: seqtrackv2_b256.yaml
New configuration is shown below.
MODEL configuration: {'HIDDEN_DIM': 256, 'BINS': 4000, 'FEATURE_TYPE': 'x', 'INTERFACE_TYPE': 'low-rank_add', 'INTERFACE_DIM': 32, 'LANGUAGE': {'IMPLEMENT': 'pytorch', 'TYPE': 'bert-base-uncased', 'PATH': 'pretrained/bert/bert-base-uncased.tar.gz', 'VOCAB_PATH': 'pretrained/bert/bert-base-uncased-vocab.txt', 'POOLING': 'mean', 'BERT': {'ENC_NUM': 12, 'HIDDEN_DIM': 256, 'MAX_QUERY_LEN': 40}}, 'ENCODER': {'TYPE': 'vitmm_base_patch16', 'DROP_PATH': 0, 'PRETRAIN_TYPE': 'mae', 'STRIDE': 16, 'USE_CHECKPOINT': False, 'INSTRUCT': True}, 'DECODER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 1024, 'DEC_LAYERS': 2, 'PRE_NORM': False, 'INSTRUCT': True}}


TRAIN configuration: {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 240, 'LR_DROP_EPOCH': 192, 'BATCH_SIZE': 32, 'NUM_WORKER': 8, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': True, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}, 'TYPE': 'peft', 'PRETRAINED_PATH': 'pretrained/seqtrack/seqtrack_b256/SEQTRACK_ep0500.pth.tar'}


DATA configuration: {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'SEQ_FORMAT': 'xywh', 'MULTI_MODAL_VISION': True, 'MULTI_MODAL_LANGUAGE': True, 'TRAIN': {'DATASETS_NAME': ['LASOT', 'REFCOCOG', 'TNL2K_train', 'OTB99_train', 'DepthTrack_train', 'VisEvent', 'LasHeR_train'], 'DATASETS_RATIO': [1, 1, 1, 1, 4, 4, 4], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 1, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 256, 'FACTOR': 4.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}


TEST configuration: {'TEMPLATE_FACTOR': 4.0, 'TEMPLATE_SIZE': 256, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 256, 'EPOCH': 240, 'WINDOW': True, 'NUM_TEMPLATES': 2, 'UPDATE_INTERVALS': {'DEFAULT': 25}, 'UPDATE_THRESHOLD': {'DEFAULT': 0.475}, 'MULTI_MODAL_VISION': {'DEFAULT': True, 'DEPTHTRACK': True, 'LASHER': True, 'VISEVENT': True, 'OTB99_LANG': True, 'TNL2K': True, 'LASOT_LANG': True}}


ðŸ”’ Using only LaSOT dataset for assignment training.
ðŸ“‚ Dataset path: /mnt/d/datasets/lasot_subset
âœ… LaSOT subset loaded successfully with 1120 sequences.
ðŸ“ Sample sequences: ['airplane-10', 'airplane-11', 'airplane-12', 'airplane-14', 'airplane-16']
Traceback (most recent call last):
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 87, in <module>
    main()
    ~~~~^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 81, in main
    run_training(args.script, args.config, cudnn_benchmark=args.cudnn_benchmark,
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 local_rank=args.local_rank, save_dir=args.save_dir, base_seed=args.seed,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 use_lmdb=args.use_lmdb)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/run_training.py", line 60, in run_training
    expr_func(settings)
    ~~~~~~~~~^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/train/train_script.py", line 56, in run
    net = build_seqtrackv2(cfg)
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 201, in build_seqtrackv2
    load_pretrained(model, cfg.TRAIN.PRETRAINED_PATH)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/saifm/SeqTrackv2/lib/train/../../lib/models/seqtrackv2/seqtrackv2.py", line 207, in load_pretrained
    seqtrackv1 = torch.load(pretrained_path, map_location="cpu")
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1546, in load
    return _legacy_load(
        opened_file,
    ...<2 lines>...
        **pickle_load_args,
    )
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/serialization.py", line 1802, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 573, in load
    return Unpickler(file, encoding=encoding).load()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/saifm/seqtrack_env/lib/python3.13/site-packages/torch/_weights_only_unpickler.py", line 325, in load
    raise EOFError
EOFError
ðŸ”’ Using fixed random seed: 7
ðŸš€ Running training command:
python lib/train/run_training.py --script seqtrackv2 --config seqtrackv2_b256 --save_dir /mnt/c/Users/saifm/SeqTrackv2/test_run --use_lmdb 0 

=== RUN END epoch 5 rc 0 time Wed Oct 15 13:14:51 2025 ===


=== RUN START epoch 1 time Wed Oct 15 13:17:33 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 1 rc 1 time Wed Oct 15 13:17:33 2025 ===


=== RUN START epoch 2 time Wed Oct 15 13:17:33 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 2 rc 1 time Wed Oct 15 13:17:34 2025 ===


=== RUN START epoch 3 time Wed Oct 15 13:17:34 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 3 rc 1 time Wed Oct 15 13:17:34 2025 ===


=== RUN START epoch 4 time Wed Oct 15 13:17:34 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 4 rc 1 time Wed Oct 15 13:17:34 2025 ===


=== RUN START epoch 5 time Wed Oct 15 13:17:34 2025 ===
Traceback (most recent call last):
  File "tracking/train.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 5 rc 1 time Wed Oct 15 13:17:35 2025 ===


=== RUN START epoch 1 time Wed Oct 15 13:16:49 2025 ===
Connected to: <socket.socket fd=3, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 64220), raddr=('127.0.0.1', 60555)>.
Traceback (most recent call last):
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'torch'

=== RUN END epoch 1 rc 1 time Wed Oct 15 13:38:11 2025 ===
